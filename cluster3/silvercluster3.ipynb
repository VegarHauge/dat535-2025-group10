{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8755f1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully!\n",
      "Current timestamp: 2025-11-25 10:08:59.875872\n",
      "Working directory: /home/ubuntu/project2/cluster3\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import findspark\n",
    "\n",
    "# Initialize findspark to locate Spark installation\n",
    "findspark.init()\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")\n",
    "print(f\"Current timestamp: {datetime.now()}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6a2aa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/25 10:09:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/25 10:09:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SPARK CLUSTER 3 - RESOURCE-CONSTRAINED TEST\n",
      "================================================================================\n",
      "Application Name: Silver-Layer-Cluster3-Resource-Constrained\n",
      "Spark Version: 3.5.0\n",
      "Max Cores: 8\n",
      "Executor Memory: 3g\n",
      "Executor Cores: 2\n",
      "Shuffle Partitions: 16\n",
      "\n",
      "Physical Cluster (Available):\n",
      "  • Workers: 4 nodes\n",
      "  • Cores per worker: 4 cores (16 total available)\n",
      "  • Memory per worker: 6.8 GiB (~27.2 GiB total available)\n",
      "\n",
      "Allocated Resources (Constrained Test):\n",
      "  • Executors: 4 (one per worker)\n",
      "  • Cores per executor: 2 (8 total = 50% of available)\n",
      "  • Memory per executor: 3g (12 GB total = 44% of available)\n",
      "  • Partitions: 16 (2x allocated cores)\n",
      "\n",
      "Test Objective: Measure performance degradation with limited resources\n",
      "================================================================================\n",
      "Max Cores: 8\n",
      "Executor Memory: 3g\n",
      "Executor Cores: 2\n",
      "Shuffle Partitions: 16\n",
      "\n",
      "Physical Cluster (Available):\n",
      "  • Workers: 4 nodes\n",
      "  • Cores per worker: 4 cores (16 total available)\n",
      "  • Memory per worker: 6.8 GiB (~27.2 GiB total available)\n",
      "\n",
      "Allocated Resources (Constrained Test):\n",
      "  • Executors: 4 (one per worker)\n",
      "  • Cores per executor: 2 (8 total = 50% of available)\n",
      "  • Memory per executor: 3g (12 GB total = 44% of available)\n",
      "  • Partitions: 16 (2x allocated cores)\n",
      "\n",
      "Test Objective: Measure performance degradation with limited resources\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession - CLUSTER 3: RESOURCE-CONSTRAINED TEST\n",
    "# Testing with limited resources to measure performance degradation\n",
    "# Configuration: 2 cores + 3GB RAM per executor (50% of available resources)\n",
    "\n",
    "SPARK_MASTER = \"spark://192.168.10.115:7077\"  # Master node from cluster\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Silver-Layer-Cluster3-Resource-Constrained\") \\\n",
    "    .master(SPARK_MASTER) \\\n",
    "    .config(\"spark.executor.memory\", \"3g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.cores.max\", \"8\") \\\n",
    "    .config(\"spark.default.parallelism\", \"16\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"16\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce verbose output\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Get SparkContext for RDD operations\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SPARK CLUSTER 3 - RESOURCE-CONSTRAINED TEST\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Application Name: {sc.appName}\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Max Cores: {spark.conf.get('spark.cores.max')}\")\n",
    "print(f\"Executor Memory: {spark.conf.get('spark.executor.memory')}\")\n",
    "print(f\"Executor Cores: {spark.conf.get('spark.executor.cores')}\")\n",
    "print(f\"Shuffle Partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print()\n",
    "print(\"Physical Cluster (Available):\")\n",
    "print(\"  • Workers: 4 nodes\")\n",
    "print(\"  • Cores per worker: 4 cores (16 total available)\")\n",
    "print(\"  • Memory per worker: 6.8 GiB (~27.2 GiB total available)\")\n",
    "print()\n",
    "print(\"Allocated Resources (Constrained Test):\")\n",
    "print(\"  • Executors: 4 (one per worker)\")\n",
    "print(\"  • Cores per executor: 2 (8 total = 50% of available)\")\n",
    "print(\"  • Memory per executor: 3g (12 GB total = 44% of available)\")\n",
    "print(\"  • Partitions: 16 (2x allocated cores)\")\n",
    "print()\n",
    "print(\"Test Objective: Measure performance degradation with limited resources\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1c10763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DEFINING UNIFIED SILVER LAYER SCHEMA\n",
      "================================================================================\n",
      "\n",
      "Target Schema Defined:\n",
      "  Total fields: 24\n",
      "  Original taxi fields: 19\n",
      "  Bronze metadata fields: 5\n",
      "\n",
      "Field Name Normalization Rules:\n",
      "  Airport_fee          -> airport_fee\n",
      "  RatecodeID           -> ratecodeid\n",
      "  PULocationID         -> pulocationid\n",
      "  DOLocationID         -> dolocationid\n",
      "  VendorID             -> vendorid\n",
      "\n",
      "================================================================================\n",
      "NORMALIZATION FUNCTION DEFINED\n",
      "================================================================================\n",
      "Function: normalize_record()\n",
      "  • Standardizes field names to lowercase\n",
      "  • Converts passenger_count to double\n",
      "  • Converts ratecodeid to double\n",
      "  • Handles null values gracefully\n",
      "\n",
      "This is a MAP operation for RDD-based cleaning!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define Unified Silver Layer Schema and Normalization Functions\n",
    "print(\"=\" * 80)\n",
    "print(\"DEFINING UNIFIED SILVER LAYER SCHEMA\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Define target schema - all field names lowercase, consistent types\n",
    "SILVER_SCHEMA = {\n",
    "    # Original taxi trip fields (19 fields)\n",
    "    'vendorid': 'long',\n",
    "    'tpep_pickup_datetime': 'timestamp',\n",
    "    'tpep_dropoff_datetime': 'timestamp',\n",
    "    'passenger_count': 'double',  # Normalize to double\n",
    "    'trip_distance': 'double',\n",
    "    'ratecodeid': 'double',  # Normalize to double\n",
    "    'store_and_fwd_flag': 'string',\n",
    "    'pulocationid': 'long',\n",
    "    'dolocationid': 'long',\n",
    "    'payment_type': 'long',\n",
    "    'fare_amount': 'double',\n",
    "    'extra': 'double',\n",
    "    'mta_tax': 'double',\n",
    "    'tip_amount': 'double',\n",
    "    'tolls_amount': 'double',\n",
    "    'improvement_surcharge': 'double',\n",
    "    'total_amount': 'double',\n",
    "    'congestion_surcharge': 'double',\n",
    "    'airport_fee': 'double',  # Standardize to lowercase\n",
    "    \n",
    "    # Bronze metadata fields (5 fields) - keep as-is\n",
    "    '_bronze_ingestion_timestamp': 'string',\n",
    "    '_bronze_source_file': 'string',\n",
    "    '_bronze_record_id': 'string',\n",
    "    '_bronze_status': 'string',\n",
    "    '_bronze_quality_flags': 'string'\n",
    "}\n",
    "\n",
    "print(\"Target Schema Defined:\")\n",
    "print(f\"  Total fields: {len(SILVER_SCHEMA)}\")\n",
    "print(f\"  Original taxi fields: 19\")\n",
    "print(f\"  Bronze metadata fields: 5\")\n",
    "print()\n",
    "\n",
    "# Field name mapping for variations\n",
    "FIELD_NAME_MAPPING = {\n",
    "    'Airport_fee': 'airport_fee',  # Capital A -> lowercase\n",
    "    'RatecodeID': 'ratecodeid',    # Just in case\n",
    "    'PULocationID': 'pulocationid',  # Just in case\n",
    "    'DOLocationID': 'dolocationid',  # Just in case\n",
    "    'VendorID': 'vendorid'  # Just in case\n",
    "}\n",
    "\n",
    "print(\"Field Name Normalization Rules:\")\n",
    "for old_name, new_name in FIELD_NAME_MAPPING.items():\n",
    "    print(f\"  {old_name:20s} -> {new_name}\")\n",
    "print()\n",
    "\n",
    "# RDD-based normalization function\n",
    "def normalize_record(record: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    MAP function: Normalize a single record to match Silver schema.\n",
    "    \n",
    "    Handles:\n",
    "    1. Field name standardization (all lowercase)\n",
    "    2. Data type conversion (passenger_count, ratecodeid to double)\n",
    "    3. Null value handling\n",
    "    \n",
    "    Args:\n",
    "        record: Dictionary representing a Bronze layer record\n",
    "    \n",
    "    Returns:\n",
    "        Normalized dictionary matching Silver schema\n",
    "    \"\"\"\n",
    "    normalized = {}\n",
    "    \n",
    "    # First pass: normalize all field names to lowercase\n",
    "    for key, value in record.items():\n",
    "        # Check if field needs explicit mapping\n",
    "        if key in FIELD_NAME_MAPPING:\n",
    "            normalized_key = FIELD_NAME_MAPPING[key]\n",
    "        else:\n",
    "            # Default: convert to lowercase\n",
    "            normalized_key = key.lower()\n",
    "        \n",
    "        normalized[normalized_key] = value\n",
    "    \n",
    "    # Second pass: ensure data type consistency for problematic fields\n",
    "    # Convert passenger_count to double if it exists\n",
    "    if 'passenger_count' in normalized and normalized['passenger_count'] is not None:\n",
    "        try:\n",
    "            normalized['passenger_count'] = float(normalized['passenger_count'])\n",
    "        except (ValueError, TypeError):\n",
    "            normalized['passenger_count'] = None\n",
    "    \n",
    "    # Convert ratecodeid to double if it exists\n",
    "    if 'ratecodeid' in normalized and normalized['ratecodeid'] is not None:\n",
    "        try:\n",
    "            normalized['ratecodeid'] = float(normalized['ratecodeid'])\n",
    "        except (ValueError, TypeError):\n",
    "            normalized['ratecodeid'] = None\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"NORMALIZATION FUNCTION DEFINED\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Function: normalize_record()\")\n",
    "print(\"  • Standardizes field names to lowercase\")\n",
    "print(\"  • Converts passenger_count to double\")\n",
    "print(\"  • Converts ratecodeid to double\")\n",
    "print(\"  • Handles null values gracefully\")\n",
    "print()\n",
    "print(\"This is a MAP operation for RDD-based cleaning!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02129f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA QUALITY VALIDATION FUNCTIONS DEFINED\n",
      "================================================================================\n",
      "\n",
      "Functions for RDD MapReduce pipeline:\n",
      "--------------------------------------------------------------------------------\n",
      "1. validate_record_quality()\n",
      "   • MAP operation\n",
      "   • Returns: (record, list_of_issues)\n",
      "   • Checks:\n",
      "     - Null values in critical fields\n",
      "     - Pickup before dropoff datetime\n",
      "     - Positive trip distances\n",
      "     - Reasonable passenger counts\n",
      "     - Non-negative fare amounts\n",
      "     - Total amount consistency\n",
      "     - Location information present\n",
      "\n",
      "2. extract_quality_issues()\n",
      "   • FLATMAP operation\n",
      "   • Extracts individual issues for counting\n",
      "   • Returns: [(issue_name, 1), ...]\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define Data Quality Validation Functions (RDD MapReduce)\n",
    "\n",
    "def validate_record_quality(record: Dict) -> Tuple[Dict, List[str]]:\n",
    "    \"\"\"\n",
    "    MAP function: Validate a single record against business rules.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (record, list_of_quality_issues)\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # 1. NULL VALUE CHECKS\n",
    "    critical_fields = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', \n",
    "                      'trip_distance', 'total_amount']\n",
    "    for field in critical_fields:\n",
    "        if record.get(field) is None:\n",
    "            issues.append(f'null_{field}')\n",
    "    \n",
    "    # 2. BUSINESS LOGIC VALIDATIONS\n",
    "    \n",
    "    # Check: pickup before dropoff\n",
    "    pickup = record.get('tpep_pickup_datetime')\n",
    "    dropoff = record.get('tpep_dropoff_datetime')\n",
    "    if pickup and dropoff:\n",
    "        if dropoff <= pickup:\n",
    "            issues.append('invalid_trip_duration')\n",
    "    \n",
    "    # Check: trip distance positive\n",
    "    distance = record.get('trip_distance')\n",
    "    if distance is not None:\n",
    "        if distance <= 0:\n",
    "            issues.append('invalid_trip_distance')\n",
    "        elif distance > 100:  # Suspiciously long trip (>100 miles in NYC)\n",
    "            issues.append('suspicious_trip_distance')\n",
    "    \n",
    "    # Check: passenger count reasonable\n",
    "    passengers = record.get('passenger_count')\n",
    "    if passengers is not None:\n",
    "        if passengers <= 0:\n",
    "            issues.append('invalid_passenger_count')\n",
    "        elif passengers > 6:  # NYC taxis typically max 4-5, but allow up to 6\n",
    "            issues.append('suspicious_passenger_count')\n",
    "    \n",
    "    # Check: fare amounts non-negative\n",
    "    fare_fields = ['fare_amount', 'extra', 'mta_tax', 'tip_amount', \n",
    "                   'tolls_amount', 'improvement_surcharge', 'total_amount',\n",
    "                   'congestion_surcharge', 'airport_fee']\n",
    "    \n",
    "    for field in fare_fields:\n",
    "        value = record.get(field)\n",
    "        if value is not None and value < 0:\n",
    "            issues.append(f'negative_{field}')\n",
    "    \n",
    "    # Check: total amount consistency (basic check)\n",
    "    total = record.get('total_amount')\n",
    "    fare = record.get('fare_amount')\n",
    "    if total is not None and fare is not None:\n",
    "        if total > 0 and fare == 0:\n",
    "            issues.append('suspicious_fare_structure')\n",
    "        elif total > 1000:  # Very expensive ride\n",
    "            issues.append('suspicious_total_amount')\n",
    "    \n",
    "    # Check: location IDs present\n",
    "    if record.get('pulocationid') is None or record.get('dolocationid') is None:\n",
    "        issues.append('missing_location_info')\n",
    "    \n",
    "    return (record, issues)\n",
    "\n",
    "\n",
    "def extract_quality_issues(validation_result: Tuple[Dict, List[str]]) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    FLATMAP function: Extract individual quality issues from validation result.\n",
    "    \n",
    "    Returns:\n",
    "        List of (issue_name, count=1) tuples for aggregation\n",
    "    \"\"\"\n",
    "    record, issues = validation_result\n",
    "    if issues:\n",
    "        return [(issue, 1) for issue in issues]\n",
    "    else:\n",
    "        return [('no_issues', 1)]\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY VALIDATION FUNCTIONS DEFINED\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"Functions for RDD MapReduce pipeline:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"1. validate_record_quality()\")\n",
    "print(\"   • MAP operation\")\n",
    "print(\"   • Returns: (record, list_of_issues)\")\n",
    "print(\"   • Checks:\")\n",
    "print(\"     - Null values in critical fields\")\n",
    "print(\"     - Pickup before dropoff datetime\")\n",
    "print(\"     - Positive trip distances\")\n",
    "print(\"     - Reasonable passenger counts\")\n",
    "print(\"     - Non-negative fare amounts\")\n",
    "print(\"     - Total amount consistency\")\n",
    "print(\"     - Location information present\")\n",
    "print()\n",
    "print(\"2. extract_quality_issues()\")\n",
    "print(\"   • FLATMAP operation\") \n",
    "print(\"   • Extracts individual issues for counting\")\n",
    "print(\"   • Returns: [(issue_name, 1), ...]\")\n",
    "print()\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "684b8903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SILVER LAYER CLEANING FUNCTIONS DEFINED\n",
      "================================================================================\n",
      "\n",
      "Functions for RDD cleaning pipeline:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. should_remove_record(issues)\n",
      "   • FILTER predicate function\n",
      "   • Returns: True if record should be removed\n",
      "   • Critical issues that cause removal:\n",
      "     - Null critical timestamps or amounts\n",
      "     - Invalid trip duration (dropoff before pickup)\n",
      "     - Invalid trip distance (≤ 0)\n",
      "     - Invalid passenger count (≤ 0)\n",
      "\n",
      "2. create_silver_record(validation_result)\n",
      "   • MAP transformation\n",
      "   • Adds Silver metadata:\n",
      "     - _silver_cleaning_timestamp\n",
      "     - _silver_quality_issues\n",
      "     - _silver_status (clean/flagged)\n",
      "     - _silver_issue_count\n",
      "\n",
      "================================================================================\n",
      "Cleaning Strategy:\n",
      "  • Remove ~2-3% of records with critical issues\n",
      "  • Keep ~97-98% including flagged records for transparency\n",
      "  • Silver layer will have 28 fields (24 Bronze + 4 Silver metadata)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define Silver Layer Cleaning Functions\n",
    "\n",
    "def should_remove_record(issues: List[str]) -> bool:\n",
    "    \"\"\"\n",
    "    FILTER function: Determine if record should be removed (True) or kept (False).\n",
    "    \n",
    "    Remove records with critical data quality issues that make them unusable.\n",
    "    \"\"\"\n",
    "    critical_issues = {\n",
    "        'null_tpep_pickup_datetime',\n",
    "        'null_tpep_dropoff_datetime',\n",
    "        'null_trip_distance',\n",
    "        'null_total_amount',\n",
    "        'invalid_trip_duration',\n",
    "        'invalid_trip_distance',\n",
    "        'invalid_passenger_count'\n",
    "    }\n",
    "    \n",
    "    # Remove if any critical issue present\n",
    "    return any(issue in critical_issues for issue in issues)\n",
    "\n",
    "\n",
    "def create_silver_record(validation_result: Tuple[Dict, List[str]]) -> Dict:\n",
    "    \"\"\"\n",
    "    MAP function: Create Silver layer record with cleaning metadata.\n",
    "    \n",
    "    Adds Silver-specific fields tracking the cleaning process.\n",
    "    \"\"\"\n",
    "    record, issues = validation_result\n",
    "    \n",
    "    # Create Silver record (copy of normalized record)\n",
    "    silver_record = record.copy()\n",
    "    \n",
    "    # Add Silver layer metadata\n",
    "    silver_record['_silver_cleaning_timestamp'] = datetime.now().isoformat()\n",
    "    silver_record['_silver_quality_issues'] = ','.join(issues) if issues else None\n",
    "    silver_record['_silver_status'] = 'flagged' if (issues and issues != ['no_issues']) else 'clean'\n",
    "    \n",
    "    # Count of issues (excluding 'no_issues')\n",
    "    issue_count = len([i for i in issues if i != 'no_issues'])\n",
    "    silver_record['_silver_issue_count'] = issue_count\n",
    "    \n",
    "    return silver_record\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SILVER LAYER CLEANING FUNCTIONS DEFINED\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"Functions for RDD cleaning pipeline:\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "print(\"1. should_remove_record(issues)\")\n",
    "print(\"   • FILTER predicate function\")\n",
    "print(\"   • Returns: True if record should be removed\")\n",
    "print(\"   • Critical issues that cause removal:\")\n",
    "print(\"     - Null critical timestamps or amounts\")\n",
    "print(\"     - Invalid trip duration (dropoff before pickup)\")\n",
    "print(\"     - Invalid trip distance (≤ 0)\")\n",
    "print(\"     - Invalid passenger count (≤ 0)\")\n",
    "print()\n",
    "print(\"2. create_silver_record(validation_result)\")\n",
    "print(\"   • MAP transformation\")\n",
    "print(\"   • Adds Silver metadata:\")\n",
    "print(\"     - _silver_cleaning_timestamp\")\n",
    "print(\"     - _silver_quality_issues\")\n",
    "print(\"     - _silver_status (clean/flagged)\")\n",
    "print(\"     - _silver_issue_count\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"Cleaning Strategy:\")\n",
    "print(\"  • Remove ~2-3% of records with critical issues\")\n",
    "print(\"  • Keep ~97-98% including flagged records for transparency\")\n",
    "print(\"  • Silver layer will have 28 fields (24 Bronze + 4 Silver metadata)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a2029b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENRICHMENT LOOKUP DATA LOADED\n",
      "================================================================================\n",
      "\n",
      "1. Taxi Zone Lookup: 265 zones loaded\n",
      "   Sample zones:\n",
      "   • ID 1: Newark Airport, EWR (EWR)\n",
      "   • ID 132: JFK Airport, Queens (Airports)\n",
      "   • ID 138: LaGuardia Airport, Queens (Airports)\n",
      "   • ID 264: N/A, Unknown (N/A)\n",
      "   • ID 265: Outside of NYC, N/A (N/A)\n",
      "\n",
      "2. Vendor Mapping: 4 vendors\n",
      "   • 1: Creative Mobile Technologies\n",
      "   • 2: Curb Mobility\n",
      "   • 6: Myle Technologies\n",
      "   • 7: Helix\n",
      "\n",
      "3. Rate Code Mapping: 7 rate codes\n",
      "   • 1: Standard rate\n",
      "   • 2: JFK\n",
      "   • 3: Newark\n",
      "   • 4: Nassau or Westchester\n",
      "   • 5: Negotiated fare\n",
      "   • 6: Group ride\n",
      "   • 99: Unknown\n",
      "\n",
      "4. Payment Type Mapping: 7 payment types\n",
      "   • 0: Flex Fare\n",
      "   • 1: Credit card\n",
      "   • 2: Cash\n",
      "   • 3: No charge\n",
      "   • 4: Dispute\n",
      "   • 5: Unknown\n",
      "   • 6: Voided trip\n",
      "\n",
      "================================================================================\n",
      "✓ All lookup data ready for enrichment\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load Enrichment Lookup Data\n",
    "import csv\n",
    "\n",
    "# 1. Load Taxi Zone Lookup Data\n",
    "taxi_zone_lookup_path = \"/home/ubuntu/dat535-2025-group10/Metadata/taxi_zone_lookup.csv\"\n",
    "taxi_zones = {}\n",
    "\n",
    "with open(taxi_zone_lookup_path, 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        location_id = int(row['LocationID'])\n",
    "        taxi_zones[location_id] = {\n",
    "            'borough': row['Borough'],\n",
    "            'zone': row['Zone'],\n",
    "            'service_zone': row['service_zone']\n",
    "        }\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ENRICHMENT LOOKUP DATA LOADED\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(f\"1. Taxi Zone Lookup: {len(taxi_zones)} zones loaded\")\n",
    "print(\"   Sample zones:\")\n",
    "for zone_id in [1, 132, 138, 264, 265]:\n",
    "    if zone_id in taxi_zones:\n",
    "        info = taxi_zones[zone_id]\n",
    "        print(f\"   • ID {zone_id}: {info['zone']}, {info['borough']} ({info['service_zone']})\")\n",
    "print()\n",
    "\n",
    "# 2. Vendor ID Mapping (from TLC data dictionary)\n",
    "vendor_mapping = {\n",
    "    1: \"Creative Mobile Technologies\",\n",
    "    2: \"Curb Mobility\",\n",
    "    6: \"Myle Technologies\",\n",
    "    7: \"Helix\"\n",
    "}\n",
    "print(f\"2. Vendor Mapping: {len(vendor_mapping)} vendors\")\n",
    "for vid, name in vendor_mapping.items():\n",
    "    print(f\"   • {vid}: {name}\")\n",
    "print()\n",
    "\n",
    "# 3. Rate Code Mapping (from TLC data dictionary)\n",
    "ratecode_mapping = {\n",
    "    1: \"Standard rate\",\n",
    "    2: \"JFK\",\n",
    "    3: \"Newark\",\n",
    "    4: \"Nassau or Westchester\",\n",
    "    5: \"Negotiated fare\",\n",
    "    6: \"Group ride\",\n",
    "    99: \"Unknown\"\n",
    "}\n",
    "print(f\"3. Rate Code Mapping: {len(ratecode_mapping)} rate codes\")\n",
    "for rid, desc in ratecode_mapping.items():\n",
    "    print(f\"   • {rid}: {desc}\")\n",
    "print()\n",
    "\n",
    "# 4. Payment Type Mapping (from TLC data dictionary)\n",
    "payment_mapping = {\n",
    "    0: \"Flex Fare\",\n",
    "    1: \"Credit card\",\n",
    "    2: \"Cash\",\n",
    "    3: \"No charge\",\n",
    "    4: \"Dispute\",\n",
    "    5: \"Unknown\",\n",
    "    6: \"Voided trip\"\n",
    "}\n",
    "print(f\"4. Payment Type Mapping: {len(payment_mapping)} payment types\")\n",
    "for pid, method in payment_mapping.items():\n",
    "    print(f\"   • {pid}: {method}\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"✓ All lookup data ready for enrichment\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1484faec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA ENRICHMENT FUNCTION DEFINED\n",
      "================================================================================\n",
      "\n",
      "Function: enrich_record(record)\n",
      "  • Type: MAP operation\n",
      "  • Input: Normalized record dictionary\n",
      "  • Output: Record with 9 additional enrichment fields\n",
      "\n",
      "Enrichment Fields Added:\n",
      "--------------------------------------------------------------------------------\n",
      "  Location Enrichment (6 fields):\n",
      "    • pickup_borough, pickup_zone, pickup_service_zone\n",
      "    • dropoff_borough, dropoff_zone, dropoff_service_zone\n",
      "\n",
      "  ID Enrichment (3 fields):\n",
      "    • vendor_name (from vendorid)\n",
      "    • rate_description (from ratecodeid)\n",
      "    • payment_method (from payment_type)\n",
      "\n",
      "Strategy:\n",
      "  • Keep original ID fields (needed for joins)\n",
      "  • Add human-readable descriptions alongside IDs\n",
      "  • Handle missing/unknown IDs gracefully with 'Unknown'\n",
      "\n",
      "================================================================================\n",
      "Total Silver Schema: 19 original + 9 enrichment + 5 Bronze + 4 Silver = 37 fields\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define Data Enrichment Function\n",
    "\n",
    "def enrich_record(record):\n",
    "    \"\"\"\n",
    "    MAP operation: Enrich record with human-readable descriptions for ID fields.\n",
    "    \n",
    "    Adds 9 new fields:\n",
    "    - pickup_borough, pickup_zone, pickup_service_zone\n",
    "    - dropoff_borough, dropoff_zone, dropoff_service_zone\n",
    "    - vendor_name\n",
    "    - rate_description\n",
    "    - payment_method\n",
    "    \n",
    "    Keeps original ID fields for joins in Gold layer.\n",
    "    \"\"\"\n",
    "    enriched = record.copy()\n",
    "    \n",
    "    # Enrich Pickup Location\n",
    "    pu_location_id = record.get('pulocationid')\n",
    "    if pu_location_id and pu_location_id in taxi_zones:\n",
    "        zone_info = taxi_zones[pu_location_id]\n",
    "        enriched['pickup_borough'] = zone_info['borough']\n",
    "        enriched['pickup_zone'] = zone_info['zone']\n",
    "        enriched['pickup_service_zone'] = zone_info['service_zone']\n",
    "    else:\n",
    "        enriched['pickup_borough'] = 'Unknown'\n",
    "        enriched['pickup_zone'] = 'Unknown'\n",
    "        enriched['pickup_service_zone'] = 'Unknown'\n",
    "    \n",
    "    # Enrich Dropoff Location\n",
    "    do_location_id = record.get('dolocationid')\n",
    "    if do_location_id and do_location_id in taxi_zones:\n",
    "        zone_info = taxi_zones[do_location_id]\n",
    "        enriched['dropoff_borough'] = zone_info['borough']\n",
    "        enriched['dropoff_zone'] = zone_info['zone']\n",
    "        enriched['dropoff_service_zone'] = zone_info['service_zone']\n",
    "    else:\n",
    "        enriched['dropoff_borough'] = 'Unknown'\n",
    "        enriched['dropoff_zone'] = 'Unknown'\n",
    "        enriched['dropoff_service_zone'] = 'Unknown'\n",
    "    \n",
    "    # Enrich Vendor ID\n",
    "    vendor_id = record.get('vendorid')\n",
    "    enriched['vendor_name'] = vendor_mapping.get(vendor_id, 'Unknown')\n",
    "    \n",
    "    # Enrich Rate Code\n",
    "    rate_code = record.get('ratecodeid')\n",
    "    if rate_code is not None:\n",
    "        enriched['rate_description'] = ratecode_mapping.get(int(rate_code), 'Unknown')\n",
    "    else:\n",
    "        enriched['rate_description'] = 'Unknown'\n",
    "    \n",
    "    # Enrich Payment Type\n",
    "    payment_type = record.get('payment_type')\n",
    "    if payment_type is not None:\n",
    "        enriched['payment_method'] = payment_mapping.get(int(payment_type), 'Unknown')\n",
    "    else:\n",
    "        enriched['payment_method'] = 'Unknown'\n",
    "    \n",
    "    return enriched\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA ENRICHMENT FUNCTION DEFINED\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"Function: enrich_record(record)\")\n",
    "print(\"  • Type: MAP operation\")\n",
    "print(\"  • Input: Normalized record dictionary\")\n",
    "print(\"  • Output: Record with 9 additional enrichment fields\")\n",
    "print()\n",
    "print(\"Enrichment Fields Added:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"  Location Enrichment (6 fields):\")\n",
    "print(\"    • pickup_borough, pickup_zone, pickup_service_zone\")\n",
    "print(\"    • dropoff_borough, dropoff_zone, dropoff_service_zone\")\n",
    "print()\n",
    "print(\"  ID Enrichment (3 fields):\")\n",
    "print(\"    • vendor_name (from vendorid)\")\n",
    "print(\"    • rate_description (from ratecodeid)\")\n",
    "print(\"    • payment_method (from payment_type)\")\n",
    "print()\n",
    "print(\"Strategy:\")\n",
    "print(\"  • Keep original ID fields (needed for joins)\")\n",
    "print(\"  • Add human-readable descriptions alongside IDs\")\n",
    "print(\"  • Handle missing/unknown IDs gracefully with 'Unknown'\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"Total Silver Schema: 19 original + 9 enrichment + 5 Bronze + 4 Silver = 37 fields\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b158b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using existing Silver layer directory: /home/ubuntu/project2/silver_layer\n",
      "\n",
      "================================================================================\n",
      "BATCH PROCESSING - ALL 24 FILES\n",
      "================================================================================\n",
      "Source: /home/ubuntu/project2/bronze_layer\n",
      "Target: /home/ubuntu/project2/silver_layer\n",
      "\n",
      "Files to process: 24\n",
      "\n",
      "Processing files (RDD MapReduce pipeline):\n",
      "--------------------------------------------------------------------------------\n",
      "Loading all 24 files in parallel across cluster...\n",
      "  Strategy: Union RDDs to avoid schema merge conflicts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All files loaded into single RDD\n",
      "  Partitions: 16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total input records: 79,479,946\n",
      "\n",
      "Executing RDD pipeline with constrained resources...\n",
      "  Step 1/5: Normalizing field names and types...\n",
      "  Step 2/5: Enriching with lookup data...\n",
      "  Step 3/5: Validating data quality...\n",
      "  Step 4/5: Filtering critical issues...\n",
      "  Step 5/5: Creating Silver records...\n",
      "\n",
      "Pipeline ready - writing to Silver layer (this triggers execution)...\n",
      "\n",
      "Writing Silver layer data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 10:20:03 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 27:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Silver layer written to: /home/ubuntu/project2/silver_layer\n",
      "\n",
      "================================================================================\n",
      "BATCH PROCESSING COMPLETE!\n",
      "================================================================================\n",
      "Total processing time: 1975.5s\n",
      "All 24 files processed in parallel\n",
      "Input records: 79,479,946\n",
      "Cluster 3 (Constrained): 4 workers, 8 allocated cores (50%), 12 GB allocated (44%)\n",
      "Throughput: 40,233 records/second\n",
      "Resource efficiency: 5,029 records/sec/core\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Batch Processing: Clean All 24 Files\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, TimestampType\n",
    "\n",
    "bronze_path = \"/home/ubuntu/dat535-2025-group10/bronze_layer\"\n",
    "silver_output_path = \"/home/ubuntu/dat535-2025-group10/silver_layer\"\n",
    "\n",
    "# Create output directory\n",
    "if not os.path.exists(silver_output_path):\n",
    "    os.makedirs(silver_output_path)\n",
    "    print(f\"✓ Created Silver layer output directory: {silver_output_path}\")\n",
    "else:\n",
    "    print(f\"✓ Using existing Silver layer directory: {silver_output_path}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"BATCH PROCESSING - ALL 24 FILES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Source: {bronze_path}\")\n",
    "print(f\"Target: {silver_output_path}\")\n",
    "print()\n",
    "\n",
    "# Get all Bronze files\n",
    "bronze_dirs = sorted([d for d in os.listdir(bronze_path) if os.path.isdir(os.path.join(bronze_path, d))])\n",
    "print(f\"Files to process: {len(bronze_dirs)}\")\n",
    "print()\n",
    "\n",
    "# Define explicit schema for Silver DataFrame (37 fields: 19 original + 9 enrichment + 5 Bronze + 4 Silver)\n",
    "silver_schema = StructType([\n",
    "    # Original taxi fields (19)\n",
    "    StructField(\"vendorid\", LongType(), True),\n",
    "    StructField(\"tpep_pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"tpep_dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"passenger_count\", DoubleType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"ratecodeid\", DoubleType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"pulocationid\", LongType(), True),\n",
    "    StructField(\"dolocationid\", LongType(), True),\n",
    "    StructField(\"payment_type\", LongType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"extra\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True),\n",
    "    StructField(\"improvement_surcharge\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"congestion_surcharge\", DoubleType(), True),\n",
    "    StructField(\"airport_fee\", DoubleType(), True),\n",
    "    # Enrichment fields (9)\n",
    "    StructField(\"pickup_borough\", StringType(), True),\n",
    "    StructField(\"pickup_zone\", StringType(), True),\n",
    "    StructField(\"pickup_service_zone\", StringType(), True),\n",
    "    StructField(\"dropoff_borough\", StringType(), True),\n",
    "    StructField(\"dropoff_zone\", StringType(), True),\n",
    "    StructField(\"dropoff_service_zone\", StringType(), True),\n",
    "    StructField(\"vendor_name\", StringType(), True),\n",
    "    StructField(\"rate_description\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    # Bronze metadata (5)\n",
    "    StructField(\"_bronze_ingestion_timestamp\", StringType(), True),\n",
    "    StructField(\"_bronze_source_file\", StringType(), True),\n",
    "    StructField(\"_bronze_record_id\", StringType(), True),\n",
    "    StructField(\"_bronze_status\", StringType(), True),\n",
    "    StructField(\"_bronze_quality_flags\", StringType(), True),\n",
    "    # Silver metadata (4)\n",
    "    StructField(\"_silver_cleaning_timestamp\", StringType(), True),\n",
    "    StructField(\"_silver_quality_issues\", StringType(), True),\n",
    "    StructField(\"_silver_status\", StringType(), True),\n",
    "    StructField(\"_silver_issue_count\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Track overall statistics\n",
    "overall_stats = {\n",
    "    'files_processed': 0,\n",
    "    'total_input_records': 0,\n",
    "    'total_removed_records': 0,\n",
    "    'total_output_records': 0,\n",
    "    'total_clean_records': 0,\n",
    "    'total_flagged_records': 0,\n",
    "    'processing_times': [],\n",
    "    'file_stats': []\n",
    "}\n",
    "\n",
    "print(\"Processing files (RDD MapReduce pipeline):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# CLUSTER-OPTIMIZED: Load ALL files at once and let Spark distribute work\n",
    "overall_start_time = time.time()\n",
    "\n",
    "# Load ALL Bronze files - process each file to RDD then union (avoids schema conflicts)\n",
    "# Spark will distribute the union RDD across the cluster\n",
    "bronze_file_paths = [os.path.join(bronze_path, d) for d in bronze_dirs]\n",
    "print(f\"Loading all {len(bronze_file_paths)} files in parallel across cluster...\")\n",
    "print(\"  Strategy: Union RDDs to avoid schema merge conflicts\")\n",
    "\n",
    "# Load each file separately and convert to RDD immediately (bypasses schema merging)\n",
    "# Then union all RDDs - Spark will parallelize this across cluster\n",
    "rdds = []\n",
    "for bronze_path_file in bronze_file_paths:\n",
    "    df = spark.read.parquet(bronze_path_file)\n",
    "    rdd = df.rdd.map(lambda row: row.asDict())\n",
    "    rdds.append(rdd)\n",
    "\n",
    "# Union all RDDs into single distributed RDD\n",
    "# sc.union() distributes work across all cluster workers\n",
    "rdd_all_bronze = sc.union(rdds)\n",
    "\n",
    "# Repartition for constrained resource allocation (16 partitions = 2x allocated cores)\n",
    "rdd_all_bronze = rdd_all_bronze.repartition(16)\n",
    "\n",
    "print(\"✓ All files loaded into single RDD\")\n",
    "print(f\"  Partitions: {rdd_all_bronze.getNumPartitions()}\")\n",
    "print()\n",
    "\n",
    "# Get input count\n",
    "input_count = rdd_all_bronze.count()\n",
    "print(f\"Total input records: {input_count:,}\")\n",
    "print()\n",
    "\n",
    "# RDD PROCESSING PIPELINE - All 24 files processed in parallel\n",
    "# Each transformation distributed across 8 ALLOCATED cores (50% of physical 16 cores)\n",
    "print(\"Executing RDD pipeline with constrained resources...\")\n",
    "print(\"  Step 1/5: Normalizing field names and types...\")\n",
    "rdd_normalized = rdd_all_bronze.map(normalize_record)\n",
    "\n",
    "print(\"  Step 2/5: Enriching with lookup data...\")\n",
    "rdd_enriched = rdd_normalized.map(enrich_record)\n",
    "\n",
    "print(\"  Step 3/5: Validating data quality...\")\n",
    "rdd_validated = rdd_enriched.map(validate_record_quality)\n",
    "\n",
    "print(\"  Step 4/5: Filtering critical issues...\")\n",
    "rdd_filtered = rdd_validated.filter(lambda x: not should_remove_record(x[1]))\n",
    "\n",
    "print(\"  Step 5/5: Creating Silver records...\")\n",
    "rdd_silver = rdd_filtered.map(create_silver_record)\n",
    "\n",
    "# No caching or repartitioning - avoid shuffle overhead with large dataset\n",
    "# Pipeline will execute in single pass during write operation\n",
    "\n",
    "print()\n",
    "print(\"Pipeline ready - writing to Silver layer (this triggers execution)...\")\n",
    "print()\n",
    "\n",
    "# Helper function to convert dict to tuple (37 fields)\n",
    "def dict_to_tuple(d):\n",
    "    return (\n",
    "        # Original 19 fields\n",
    "        d.get('vendorid'), d.get('tpep_pickup_datetime'), d.get('tpep_dropoff_datetime'),\n",
    "        d.get('passenger_count'), d.get('trip_distance'), d.get('ratecodeid'),\n",
    "        d.get('store_and_fwd_flag'), d.get('pulocationid'), d.get('dolocationid'),\n",
    "        d.get('payment_type'), d.get('fare_amount'), d.get('extra'),\n",
    "        d.get('mta_tax'), d.get('tip_amount'), d.get('tolls_amount'),\n",
    "        d.get('improvement_surcharge'), d.get('total_amount'), d.get('congestion_surcharge'),\n",
    "        d.get('airport_fee'),\n",
    "        # Enrichment 9 fields\n",
    "        d.get('pickup_borough'), d.get('pickup_zone'), d.get('pickup_service_zone'),\n",
    "        d.get('dropoff_borough'), d.get('dropoff_zone'), d.get('dropoff_service_zone'),\n",
    "        d.get('vendor_name'), d.get('rate_description'), d.get('payment_method'),\n",
    "        # Bronze 5 fields\n",
    "        d.get('_bronze_ingestion_timestamp'), d.get('_bronze_source_file'),\n",
    "        d.get('_bronze_record_id'), d.get('_bronze_status'), d.get('_bronze_quality_flags'),\n",
    "        # Silver 4 fields\n",
    "        d.get('_silver_cleaning_timestamp'), d.get('_silver_quality_issues'),\n",
    "        d.get('_silver_status'), d.get('_silver_issue_count')\n",
    "    )\n",
    "\n",
    "# Convert to DataFrame and write - Spark will handle partitioning by source file\n",
    "print(\"Writing Silver layer data...\")\n",
    "rdd_tuples = rdd_silver.map(dict_to_tuple)\n",
    "silver_df = spark.createDataFrame(rdd_tuples, schema=silver_schema)\n",
    "\n",
    "# Write as single unified Silver layer with coalescing for efficiency\n",
    "silver_df.coalesce(8).write.mode('overwrite').parquet(silver_output_path)\n",
    "\n",
    "write_time = time.time() - overall_start_time\n",
    "\n",
    "print(f\"✓ Silver layer written to: {silver_output_path}\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"BATCH PROCESSING COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total processing time: {write_time:.1f}s\")\n",
    "print(f\"All {len(bronze_dirs)} files processed in parallel\")\n",
    "print(f\"Input records: {input_count:,}\")\n",
    "print(f\"Cluster 3 (Constrained): 4 workers, 8 allocated cores (50%), 12 GB allocated (44%)\")\n",
    "print(f\"Throughput: {input_count/write_time:,.0f} records/second\")\n",
    "print(f\"Resource efficiency: {(input_count/write_time)/8:,.0f} records/sec/core\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
