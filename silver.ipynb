{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50365aef",
   "metadata": {},
   "source": [
    "# Silver Layer Implementation - NYC Yellow Taxi Data Pipeline\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements the **Silver Layer** of the Medallion Architecture for NYC Yellow Taxi Trip data. The Silver layer focuses on **data cleaning, validation, and business rule enforcement**.\n",
    "\n",
    "### Medallion Architecture - Silver Layer Goals\n",
    "The Silver layer serves as the **cleaned and validated data zone** where we:\n",
    "1. Load Bronze layer data with quality flags\n",
    "2. Perform comprehensive data quality analysis\n",
    "3. Apply business rule validations\n",
    "4. Handle missing and invalid values\n",
    "5. Create clean, trustworthy datasets for analytics\n",
    "6. Document all cleaning decisions with justification\n",
    "\n",
    "### Dataset Context\n",
    "- **Source**: Bronze layer output (79.5M records, 24 monthly files)\n",
    "- **Input Location**: `/home/ubuntu/dat535-2025-group10/bronze_layer/`\n",
    "- **Quality Baseline**: 89.65% clean, 10.35% flagged in Bronze\n",
    "- **Environment**: Single VM with 4 vCPUs, 8GB RAM, 40GB storage\n",
    "\n",
    "### Silver Layer Approach\n",
    "Following **Part 2 of project.md**, we will:\n",
    "- Identify cleaning/preprocessing steps through data profiling\n",
    "- **Implement cleaning using basic MapReduce routines in Spark**\n",
    "- Profile and tune the implementation for performance\n",
    "- Result: Clean dataset ready for Gold layer analytics\n",
    "\n",
    "### Critical Constraint from project.md Part 2:\n",
    "**\"Implement cleaning/preprocessing steps using basic MapReduce routines in Spark (no SQL, Dataframes or similar libraries)\"**\n",
    "\n",
    "This means we must use RDD operations (map, flatMap, filter, reduce, reduceByKey, etc.) for all data cleaning logic, similar to the Bronze layer approach.\n",
    "\n",
    "### Learning Objectives\n",
    "- Systematic data quality assessment using MapReduce\n",
    "- Business rule validation with RDD transformations\n",
    "- Missing value handling using map/filter operations\n",
    "- Outlier detection with reduce operations\n",
    "- Performance optimization for RDD-based cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db5f7a",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup and Bronze Layer Loading\n",
    "\n",
    "We'll start by:\n",
    "1. Importing necessary libraries\n",
    "2. Initializing SparkSession with appropriate configurations\n",
    "3. Loading Bronze layer data\n",
    "4. Understanding the current state of data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32be9635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully!\n",
      "Current timestamp: 2025-11-22 10:18:11.511455\n",
      "Working directory: /home/ubuntu/project2\n",
      "\n",
      "NOTE: Silver layer will use RDD operations (MapReduce) for data cleaning\n",
      "      per Part 2 requirements: 'no SQL, Dataframes or similar libraries'\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import findspark\n",
    "\n",
    "# Initialize findspark to locate Spark installation\n",
    "findspark.init()\n",
    "\n",
    "# PySpark imports - We'll use RDD operations for Silver layer cleaning\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")\n",
    "print(f\"Current timestamp: {datetime.now()}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print()\n",
    "print(\"NOTE: Silver layer will use RDD operations (MapReduce) for data cleaning\")\n",
    "print(\"      per Part 2 requirements: 'no SQL, Dataframes or similar libraries'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44e9cd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/22 10:18:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/22 10:18:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SPARK SESSION INITIALIZED FOR SILVER LAYER (RDD-BASED CLEANING)\n",
      "================================================================================\n",
      "Application Name: Silver-Layer-RDD-Data-Cleaning\n",
      "Spark Version: 3.5.0\n",
      "Master: local[*]\n",
      "Default Parallelism: 8\n",
      "Driver Memory: 4g\n",
      "Executor Memory: 2g\n",
      "Adaptive Execution: true\n",
      "Shuffle Partitions: 8\n",
      "\n",
      "Processing Mode: RDD MapReduce operations (per Part 2 requirements)\n",
      "================================================================================\n",
      "Driver Memory: 4g\n",
      "Executor Memory: 2g\n",
      "Adaptive Execution: true\n",
      "Shuffle Partitions: 8\n",
      "\n",
      "Processing Mode: RDD MapReduce operations (per Part 2 requirements)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession optimized for RDD-based data cleaning\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Silver-Layer-RDD-Data-Cleaning\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.default.parallelism\", \"8\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce verbose output\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Get SparkContext for RDD operations\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SPARK SESSION INITIALIZED FOR SILVER LAYER (RDD-BASED CLEANING)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Application Name: {sc.appName}\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Master: {sc.master}\")\n",
    "print(f\"Default Parallelism: {sc.defaultParallelism}\")\n",
    "print(f\"Driver Memory: {spark.conf.get('spark.driver.memory')}\")\n",
    "print(f\"Executor Memory: {spark.conf.get('spark.executor.memory')}\")\n",
    "print(f\"Adaptive Execution: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
    "print(f\"Shuffle Partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print()\n",
    "print(\"Processing Mode: RDD MapReduce operations (per Part 2 requirements)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96c90c7",
   "metadata": {},
   "source": [
    "### Reflection on Spark Initialization\n",
    "\n",
    "**What we accomplished:**\n",
    "- Successfully created SparkSession with application name \"Silver-Layer-RDD-Data-Cleaning\"\n",
    "- Using Spark 3.5.0 on local[*] mode (utilizing all 4 vCPUs)\n",
    "- Memory configuration: 4GB driver, 2GB executor (same as Bronze layer)\n",
    "- Obtained SparkContext reference for RDD operations\n",
    "- Optimized settings for RDD-based data cleaning:\n",
    "  - Default parallelism: 8 partitions (2x vCPUs for optimal CPU utilization)\n",
    "  - Same configuration as Bronze layer for consistency\n",
    "\n",
    "**Configuration rationale:**\n",
    "- Same memory footprint as Bronze layer since we're processing the same 79.5M records\n",
    "- RDD operations will use default parallelism of 8 for distributed processing\n",
    "- All cleaning logic will use MapReduce paradigm (map, filter, flatMap, reduce, etc.)\n",
    "\n",
    "**Next step:**\n",
    "Load the Bronze layer data using DataFrame (for efficient Parquet reading), then immediately convert to RDD for all cleaning operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d563280",
   "metadata": {},
   "source": [
    "## Part 1.5: Schema Normalization\n",
    "\n",
    "The Bronze layer contains schema inconsistencies that must be resolved:\n",
    "- **Field name variations:** `Airport_fee` vs `airport_fee` (capitalization differences)\n",
    "- **Data type inconsistencies:** `passenger_count` and `ratecodeid` appear as both DoubleType and LongType\n",
    "\n",
    "Solution: Create unified schema and normalization MAP function to standardize all records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aec7ff",
   "metadata": {},
   "source": [
    "### Schema Normalization Implementation\n",
    "\n",
    "**Objective:**\n",
    "Create unified schema and normalization MAP function to standardize all Bronze records.\n",
    "\n",
    "**Schema Inconsistencies Identified:**\n",
    "\n",
    "**1. Field Name Variations:**\n",
    "- `Airport_fee` (capital A) vs `airport_fee` (lowercase) - needs standardization\n",
    "\n",
    "**2. Data Type Variations:**\n",
    "- `passenger_count`: DoubleType vs LongType across different months\n",
    "- `ratecodeid`: DoubleType vs LongType across different months\n",
    "\n",
    "**Solution:**\n",
    "Define a unified target schema and create RDD-based `normalize_record()` function to:\n",
    "- Standardize all field names (lowercase)\n",
    "- Convert all numeric fields to Double for consistency\n",
    "- Handle any null/missing values appropriately\n",
    "\n",
    "**Implementation:**\n",
    "The normalization function is a **pure MAP transformation** - no side effects, just record transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34511170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DEFINING UNIFIED SILVER LAYER SCHEMA\n",
      "================================================================================\n",
      "\n",
      "Target Schema Defined:\n",
      "  Total fields: 24\n",
      "  Original taxi fields: 19\n",
      "  Bronze metadata fields: 5\n",
      "\n",
      "Field Name Normalization Rules:\n",
      "  Airport_fee          -> airport_fee\n",
      "  RatecodeID           -> ratecodeid\n",
      "  PULocationID         -> pulocationid\n",
      "  DOLocationID         -> dolocationid\n",
      "  VendorID             -> vendorid\n",
      "\n",
      "================================================================================\n",
      "NORMALIZATION FUNCTION DEFINED\n",
      "================================================================================\n",
      "Function: normalize_record()\n",
      "  • Standardizes field names to lowercase\n",
      "  • Converts passenger_count to double\n",
      "  • Converts ratecodeid to double\n",
      "  • Handles null values gracefully\n",
      "\n",
      "This is a MAP operation for RDD-based cleaning!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define Unified Silver Layer Schema and Normalization Functions\n",
    "print(\"=\" * 80)\n",
    "print(\"DEFINING UNIFIED SILVER LAYER SCHEMA\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Define target schema - all field names lowercase, consistent types\n",
    "SILVER_SCHEMA = {\n",
    "    # Original taxi trip fields (19 fields)\n",
    "    'vendorid': 'long',\n",
    "    'tpep_pickup_datetime': 'timestamp',\n",
    "    'tpep_dropoff_datetime': 'timestamp',\n",
    "    'passenger_count': 'double',  # Normalize to double\n",
    "    'trip_distance': 'double',\n",
    "    'ratecodeid': 'double',  # Normalize to double\n",
    "    'store_and_fwd_flag': 'string',\n",
    "    'pulocationid': 'long',\n",
    "    'dolocationid': 'long',\n",
    "    'payment_type': 'long',\n",
    "    'fare_amount': 'double',\n",
    "    'extra': 'double',\n",
    "    'mta_tax': 'double',\n",
    "    'tip_amount': 'double',\n",
    "    'tolls_amount': 'double',\n",
    "    'improvement_surcharge': 'double',\n",
    "    'total_amount': 'double',\n",
    "    'congestion_surcharge': 'double',\n",
    "    'airport_fee': 'double',  # Standardize to lowercase\n",
    "    \n",
    "    # Bronze metadata fields (5 fields) - keep as-is\n",
    "    '_bronze_ingestion_timestamp': 'string',\n",
    "    '_bronze_source_file': 'string',\n",
    "    '_bronze_record_id': 'string',\n",
    "    '_bronze_status': 'string',\n",
    "    '_bronze_quality_flags': 'string'\n",
    "}\n",
    "\n",
    "print(\"Target Schema Defined:\")\n",
    "print(f\"  Total fields: {len(SILVER_SCHEMA)}\")\n",
    "print(f\"  Original taxi fields: 19\")\n",
    "print(f\"  Bronze metadata fields: 5\")\n",
    "print()\n",
    "\n",
    "# Field name mapping for variations\n",
    "FIELD_NAME_MAPPING = {\n",
    "    'Airport_fee': 'airport_fee',  # Capital A -> lowercase\n",
    "    'RatecodeID': 'ratecodeid',    # Just in case\n",
    "    'PULocationID': 'pulocationid',  # Just in case\n",
    "    'DOLocationID': 'dolocationid',  # Just in case\n",
    "    'VendorID': 'vendorid'  # Just in case\n",
    "}\n",
    "\n",
    "print(\"Field Name Normalization Rules:\")\n",
    "for old_name, new_name in FIELD_NAME_MAPPING.items():\n",
    "    print(f\"  {old_name:20s} -> {new_name}\")\n",
    "print()\n",
    "\n",
    "# RDD-based normalization function\n",
    "def normalize_record(record: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    MAP function: Normalize a single record to match Silver schema.\n",
    "    \n",
    "    Handles:\n",
    "    1. Field name standardization (all lowercase)\n",
    "    2. Data type conversion (passenger_count, ratecodeid to double)\n",
    "    3. Null value handling\n",
    "    \n",
    "    Args:\n",
    "        record: Dictionary representing a Bronze layer record\n",
    "    \n",
    "    Returns:\n",
    "        Normalized dictionary matching Silver schema\n",
    "    \"\"\"\n",
    "    normalized = {}\n",
    "    \n",
    "    # First pass: normalize all field names to lowercase\n",
    "    for key, value in record.items():\n",
    "        # Check if field needs explicit mapping\n",
    "        if key in FIELD_NAME_MAPPING:\n",
    "            normalized_key = FIELD_NAME_MAPPING[key]\n",
    "        else:\n",
    "            # Default: convert to lowercase\n",
    "            normalized_key = key.lower()\n",
    "        \n",
    "        normalized[normalized_key] = value\n",
    "    \n",
    "    # Second pass: ensure data type consistency for problematic fields\n",
    "    # Convert passenger_count to double if it exists\n",
    "    if 'passenger_count' in normalized and normalized['passenger_count'] is not None:\n",
    "        try:\n",
    "            normalized['passenger_count'] = float(normalized['passenger_count'])\n",
    "        except (ValueError, TypeError):\n",
    "            normalized['passenger_count'] = None\n",
    "    \n",
    "    # Convert ratecodeid to double if it exists\n",
    "    if 'ratecodeid' in normalized and normalized['ratecodeid'] is not None:\n",
    "        try:\n",
    "            normalized['ratecodeid'] = float(normalized['ratecodeid'])\n",
    "        except (ValueError, TypeError):\n",
    "            normalized['ratecodeid'] = None\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"NORMALIZATION FUNCTION DEFINED\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Function: normalize_record()\")\n",
    "print(\"  • Standardizes field names to lowercase\")\n",
    "print(\"  • Converts passenger_count to double\")\n",
    "print(\"  • Converts ratecodeid to double\")\n",
    "print(\"  • Handles null values gracefully\")\n",
    "print()\n",
    "print(\"This is a MAP operation for RDD-based cleaning!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5e5736",
   "metadata": {},
   "source": [
    "### Reflection on Schema Normalization\n",
    "\n",
    "**What we accomplished:**\n",
    "Successfully created RDD-based schema normalization function that standardizes heterogeneous Bronze schemas.\n",
    "\n",
    "**Key Normalizations:**\n",
    "- `Airport_fee` → `airport_fee`\n",
    "- `VendorID` → `vendorid`\n",
    "- `PULocationID` → `pulocationid`\n",
    "- `DOLocationID` → `dolocationid`\n",
    "- `RatecodeID` → `ratecodeid`\n",
    "- `passenger_count`: converted to float for consistency\n",
    "\n",
    "**MapReduce Operation:**\n",
    "The `normalize_record()` function is a **MAP transformation** that takes each record (dictionary) and returns a normalized version. This is pure functional programming - no side effects, just transformation.\n",
    "\n",
    "**Next Step:**\n",
    "Implement data enrichment to add human-readable fields for ID lookups, followed by quality validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6873cc",
   "metadata": {},
   "source": [
    "## Part 2: Data Quality Validation with RDD Operations\n",
    "\n",
    "Now we'll implement comprehensive data cleaning using MapReduce operations:\n",
    "1. **Null value analysis** - Identify missing data patterns\n",
    "2. **Business rule validation** - Check fare amounts, trip distances, timestamps\n",
    "3. **Quality flag generation** - Mark records needing attention\n",
    "4. **Statistical profiling** - Understand data distributions\n",
    "\n",
    "All using RDD operations: map, filter, flatMap, reduce, reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3938dff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA QUALITY VALIDATION FUNCTIONS DEFINED\n",
      "================================================================================\n",
      "\n",
      "Functions for RDD MapReduce pipeline:\n",
      "--------------------------------------------------------------------------------\n",
      "1. validate_record_quality()\n",
      "   • MAP operation\n",
      "   • Returns: (record, list_of_issues)\n",
      "   • Checks:\n",
      "     - Null values in critical fields\n",
      "     - Pickup before dropoff datetime\n",
      "     - Positive trip distances\n",
      "     - Reasonable passenger counts\n",
      "     - Non-negative fare amounts\n",
      "     - Total amount consistency\n",
      "     - Location information present\n",
      "\n",
      "2. extract_quality_issues()\n",
      "   • FLATMAP operation\n",
      "   • Extracts individual issues for counting\n",
      "   • Returns: [(issue_name, 1), ...]\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define Data Quality Validation Functions (RDD MapReduce)\n",
    "\n",
    "def validate_record_quality(record: Dict) -> Tuple[Dict, List[str]]:\n",
    "    \"\"\"\n",
    "    MAP function: Validate a single record against business rules.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (record, list_of_quality_issues)\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # 1. NULL VALUE CHECKS\n",
    "    critical_fields = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', \n",
    "                      'trip_distance', 'total_amount']\n",
    "    for field in critical_fields:\n",
    "        if record.get(field) is None:\n",
    "            issues.append(f'null_{field}')\n",
    "    \n",
    "    # 2. BUSINESS LOGIC VALIDATIONS\n",
    "    \n",
    "    # Check: pickup before dropoff\n",
    "    pickup = record.get('tpep_pickup_datetime')\n",
    "    dropoff = record.get('tpep_dropoff_datetime')\n",
    "    if pickup and dropoff:\n",
    "        if dropoff <= pickup:\n",
    "            issues.append('invalid_trip_duration')\n",
    "    \n",
    "    # Check: trip distance positive\n",
    "    distance = record.get('trip_distance')\n",
    "    if distance is not None:\n",
    "        if distance <= 0:\n",
    "            issues.append('invalid_trip_distance')\n",
    "        elif distance > 100:  # Suspiciously long trip (>100 miles in NYC)\n",
    "            issues.append('suspicious_trip_distance')\n",
    "    \n",
    "    # Check: passenger count reasonable\n",
    "    passengers = record.get('passenger_count')\n",
    "    if passengers is not None:\n",
    "        if passengers <= 0:\n",
    "            issues.append('invalid_passenger_count')\n",
    "        elif passengers > 6:  # NYC taxis typically max 4-5, but allow up to 6\n",
    "            issues.append('suspicious_passenger_count')\n",
    "    \n",
    "    # Check: fare amounts non-negative\n",
    "    fare_fields = ['fare_amount', 'extra', 'mta_tax', 'tip_amount', \n",
    "                   'tolls_amount', 'improvement_surcharge', 'total_amount',\n",
    "                   'congestion_surcharge', 'airport_fee']\n",
    "    \n",
    "    for field in fare_fields:\n",
    "        value = record.get(field)\n",
    "        if value is not None and value < 0:\n",
    "            issues.append(f'negative_{field}')\n",
    "    \n",
    "    # Check: total amount consistency (basic check)\n",
    "    total = record.get('total_amount')\n",
    "    fare = record.get('fare_amount')\n",
    "    if total is not None and fare is not None:\n",
    "        if total > 0 and fare == 0:\n",
    "            issues.append('suspicious_fare_structure')\n",
    "        elif total > 1000:  # Very expensive ride\n",
    "            issues.append('suspicious_total_amount')\n",
    "    \n",
    "    # Check: location IDs present\n",
    "    if record.get('pulocationid') is None or record.get('dolocationid') is None:\n",
    "        issues.append('missing_location_info')\n",
    "    \n",
    "    return (record, issues)\n",
    "\n",
    "\n",
    "def extract_quality_issues(validation_result: Tuple[Dict, List[str]]) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    FLATMAP function: Extract individual quality issues from validation result.\n",
    "    \n",
    "    Returns:\n",
    "        List of (issue_name, count=1) tuples for aggregation\n",
    "    \"\"\"\n",
    "    record, issues = validation_result\n",
    "    if issues:\n",
    "        return [(issue, 1) for issue in issues]\n",
    "    else:\n",
    "        return [('no_issues', 1)]\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY VALIDATION FUNCTIONS DEFINED\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"Functions for RDD MapReduce pipeline:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"1. validate_record_quality()\")\n",
    "print(\"   • MAP operation\")\n",
    "print(\"   • Returns: (record, list_of_issues)\")\n",
    "print(\"   • Checks:\")\n",
    "print(\"     - Null values in critical fields\")\n",
    "print(\"     - Pickup before dropoff datetime\")\n",
    "print(\"     - Positive trip distances\")\n",
    "print(\"     - Reasonable passenger counts\")\n",
    "print(\"     - Non-negative fare amounts\")\n",
    "print(\"     - Total amount consistency\")\n",
    "print(\"     - Location information present\")\n",
    "print()\n",
    "print(\"2. extract_quality_issues()\")\n",
    "print(\"   • FLATMAP operation\") \n",
    "print(\"   • Extracts individual issues for counting\")\n",
    "print(\"   • Returns: [(issue_name, 1), ...]\")\n",
    "print()\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddeee6e",
   "metadata": {},
   "source": [
    "### Reflection on Quality Validation\n",
    "\n",
    "**What we accomplished:**\n",
    "Successfully implemented comprehensive data quality validation using pure RDD MapReduce operations!\n",
    "\n",
    "**Quality Validation Rules (14 total):**\n",
    "\n",
    "**Critical Issues (Records Removed):**\n",
    "1. Null critical timestamps or amounts\n",
    "2. Invalid trip duration (dropoff before pickup)\n",
    "3. Invalid trip distance (≤ 0 miles)\n",
    "4. Invalid passenger count (≤ 0 passengers)\n",
    "\n",
    "**Non-Critical Issues (Records Flagged):**\n",
    "- Negative fare amounts (might be refunds/corrections)\n",
    "- Suspicious values (outliers)\n",
    "- Missing location information\n",
    "\n",
    "**MapReduce Pipeline:**\n",
    "```\n",
    "Raw Data → map(normalize) → map(enrich) → map(validate) → filter(!remove) → map(add_metadata)\n",
    "```\n",
    "\n",
    "All operations are RDD-based transformations, following Part 2 requirements perfectly!\n",
    "\n",
    "**Next Step:**\n",
    "Process all 24 files using the complete pipeline including enrichment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b38acc",
   "metadata": {},
   "source": [
    "## Part 3: Data Cleaning Strategy and Implementation\n",
    "\n",
    "Based on quality analysis, we'll implement a cleaning strategy using RDD filter operations:\n",
    "\n",
    "**Cleaning Approach:**\n",
    "1. **Remove records with critical issues:**\n",
    "   - Invalid trip duration (dropoff before pickup)\n",
    "   - Zero/negative trip distance\n",
    "   - Zero/negative passenger count\n",
    "   - Missing critical timestamps\n",
    "\n",
    "2. **Keep but flag suspicious records:**\n",
    "   - Negative fare amounts (might be refunds/corrections)\n",
    "   - Very high amounts or distances (outliers but potentially valid)\n",
    "   \n",
    "3. **Add Silver layer metadata:**\n",
    "   - Cleaning timestamp\n",
    "   - Issues detected\n",
    "   - Clean/flagged status\n",
    "\n",
    "This follows Medallion Architecture: Bronze preserves all, Silver cleans and validates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d72ccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SILVER LAYER CLEANING FUNCTIONS DEFINED\n",
      "================================================================================\n",
      "\n",
      "Functions for RDD cleaning pipeline:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. should_remove_record(issues)\n",
      "   • FILTER predicate function\n",
      "   • Returns: True if record should be removed\n",
      "   • Critical issues that cause removal:\n",
      "     - Null critical timestamps or amounts\n",
      "     - Invalid trip duration (dropoff before pickup)\n",
      "     - Invalid trip distance (≤ 0)\n",
      "     - Invalid passenger count (≤ 0)\n",
      "\n",
      "2. create_silver_record(validation_result)\n",
      "   • MAP transformation\n",
      "   • Adds Silver metadata:\n",
      "     - _silver_cleaning_timestamp\n",
      "     - _silver_quality_issues\n",
      "     - _silver_status (clean/flagged)\n",
      "     - _silver_issue_count\n",
      "\n",
      "================================================================================\n",
      "Cleaning Strategy:\n",
      "  • Remove ~2-3% of records with critical issues\n",
      "  • Keep ~97-98% including flagged records for transparency\n",
      "  • Silver layer will have 28 fields (24 Bronze + 4 Silver metadata)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define Silver Layer Cleaning Functions\n",
    "\n",
    "def should_remove_record(issues: List[str]) -> bool:\n",
    "    \"\"\"\n",
    "    FILTER function: Determine if record should be removed (True) or kept (False).\n",
    "    \n",
    "    Remove records with critical data quality issues that make them unusable.\n",
    "    \"\"\"\n",
    "    critical_issues = {\n",
    "        'null_tpep_pickup_datetime',\n",
    "        'null_tpep_dropoff_datetime',\n",
    "        'null_trip_distance',\n",
    "        'null_total_amount',\n",
    "        'invalid_trip_duration',\n",
    "        'invalid_trip_distance',\n",
    "        'invalid_passenger_count'\n",
    "    }\n",
    "    \n",
    "    # Remove if any critical issue present\n",
    "    return any(issue in critical_issues for issue in issues)\n",
    "\n",
    "\n",
    "def create_silver_record(validation_result: Tuple[Dict, List[str]]) -> Dict:\n",
    "    \"\"\"\n",
    "    MAP function: Create Silver layer record with cleaning metadata.\n",
    "    \n",
    "    Adds Silver-specific fields tracking the cleaning process.\n",
    "    \"\"\"\n",
    "    record, issues = validation_result\n",
    "    \n",
    "    # Create Silver record (copy of normalized record)\n",
    "    silver_record = record.copy()\n",
    "    \n",
    "    # Add Silver layer metadata\n",
    "    silver_record['_silver_cleaning_timestamp'] = datetime.now().isoformat()\n",
    "    silver_record['_silver_quality_issues'] = ','.join(issues) if issues else None\n",
    "    silver_record['_silver_status'] = 'flagged' if (issues and issues != ['no_issues']) else 'clean'\n",
    "    \n",
    "    # Count of issues (excluding 'no_issues')\n",
    "    issue_count = len([i for i in issues if i != 'no_issues'])\n",
    "    silver_record['_silver_issue_count'] = issue_count\n",
    "    \n",
    "    return silver_record\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SILVER LAYER CLEANING FUNCTIONS DEFINED\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"Functions for RDD cleaning pipeline:\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "print(\"1. should_remove_record(issues)\")\n",
    "print(\"   • FILTER predicate function\")\n",
    "print(\"   • Returns: True if record should be removed\")\n",
    "print(\"   • Critical issues that cause removal:\")\n",
    "print(\"     - Null critical timestamps or amounts\")\n",
    "print(\"     - Invalid trip duration (dropoff before pickup)\")\n",
    "print(\"     - Invalid trip distance (≤ 0)\")\n",
    "print(\"     - Invalid passenger count (≤ 0)\")\n",
    "print()\n",
    "print(\"2. create_silver_record(validation_result)\")\n",
    "print(\"   • MAP transformation\")\n",
    "print(\"   • Adds Silver metadata:\")\n",
    "print(\"     - _silver_cleaning_timestamp\")\n",
    "print(\"     - _silver_quality_issues\")\n",
    "print(\"     - _silver_status (clean/flagged)\")\n",
    "print(\"     - _silver_issue_count\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"Cleaning Strategy:\")\n",
    "print(\"  • Remove ~2-3% of records with critical issues\")\n",
    "print(\"  • Keep ~97-98% including flagged records for transparency\")\n",
    "print(\"  • Silver layer will have 28 fields (24 Bronze + 4 Silver metadata)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e50d1e",
   "metadata": {},
   "source": [
    "### Reflection on Complete Silver Layer Pipeline\n",
    "\n",
    "**What we accomplished:**\n",
    "Successfully implemented complete data cleaning AND enrichment pipeline using **exclusively RDD MapReduce operations**!\n",
    "\n",
    "**Complete Pipeline:**\n",
    "```\n",
    "Bronze Data (Parquet)\n",
    "  ↓ Load & Convert to RDD\n",
    "  ↓ MAP: normalize_record() ← Schema standardization\n",
    "  ↓ MAP: enrich_record() ← Add human-readable fields\n",
    "  ↓ MAP: validate_record_quality() ← Quality validation\n",
    "  ↓ FILTER: !should_remove_record() ← Remove critical issues\n",
    "  ↓ MAP: create_silver_record() ← Add Silver metadata\n",
    "  ↓ Silver Data (Clean + Enriched + Flagged)\n",
    "```\n",
    "\n",
    "**Schema Transformation:**\n",
    "-  All field names standardized to lowercase\n",
    "-  Data types normalized (passenger_count, ratecodeid → Double)\n",
    "-  **9 enrichment fields added** (locations, vendor, rate, payment)\n",
    "-  4 Silver metadata fields added\n",
    "-  **Final schema: 37 fields** (19 original + 9 enrichment + 5 Bronze + 4 Silver)\n",
    "\n",
    "**Quality Strategy:**\n",
    "- **Critical Issues**: Removed (3.15% of records) - data is unusable\n",
    "- **Non-Critical Issues**: Flagged (1.34% of kept records) - data kept for analysis\n",
    "- **Clean Records**: 98.66% of kept records have no issues\n",
    "\n",
    "**Silver Layer Metadata:**\n",
    "- `_silver_cleaning_timestamp`: When cleaning occurred\n",
    "- `_silver_quality_issues`: Comma-separated list of issues found\n",
    "- `_silver_status`: 'clean' or 'flagged'\n",
    "- `_silver_issue_count`: Number of quality issues\n",
    "\n",
    "**Part 2 Requirements Met:**\n",
    " All cleaning implemented using basic MapReduce routines\n",
    " No SQL, DataFrames, or similar libraries used for processing\n",
    "map(), filter(), flatMap(), reduceByKey() operations demonstrated\n",
    " Result converted to DataFrame for output\n",
    "\n",
    "**Next Step:**\n",
    "Batch process all 24 files with complete enriched pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd86d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3B: Data Enrichment (Adding Human-Readable Fields)\n",
    "\n",
    "Before validation, we'll enrich the data by adding human-readable descriptions for ID fields.\n",
    "\n",
    "**Enrichment Strategy:**\n",
    "- Load lookup data (taxi zones, vendor names, rate codes, payment types)\n",
    "- Create enrichment MAP function to add descriptive fields\n",
    "- Keep original IDs (needed for joins) + add new descriptive fields\n",
    "- Apply AFTER normalization, BEFORE validation in pipeline\n",
    "\n",
    "**Fields to Enrich:**\n",
    "1. **Location IDs** → Borough, Zone, Service Zone (from taxi_zone_lookup.csv)\n",
    "2. **Vendor ID** → Vendor Name\n",
    "3. **Rate Code ID** → Rate Description\n",
    "4. **Payment Type** → Payment Method\n",
    "\n",
    "This follows best practice: Silver layer = cleaned + enriched data ready for analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d1d39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENRICHMENT LOOKUP DATA LOADED\n",
      "================================================================================\n",
      "\n",
      "1. Taxi Zone Lookup: 265 zones loaded\n",
      "   Sample zones:\n",
      "   • ID 1: Newark Airport, EWR (EWR)\n",
      "   • ID 132: JFK Airport, Queens (Airports)\n",
      "   • ID 138: LaGuardia Airport, Queens (Airports)\n",
      "   • ID 264: N/A, Unknown (N/A)\n",
      "   • ID 265: Outside of NYC, N/A (N/A)\n",
      "\n",
      "2. Vendor Mapping: 4 vendors\n",
      "   • 1: Creative Mobile Technologies\n",
      "   • 2: Curb Mobility\n",
      "   • 6: Myle Technologies\n",
      "   • 7: Helix\n",
      "\n",
      "3. Rate Code Mapping: 7 rate codes\n",
      "   • 1: Standard rate\n",
      "   • 2: JFK\n",
      "   • 3: Newark\n",
      "   • 4: Nassau or Westchester\n",
      "   • 5: Negotiated fare\n",
      "   • 6: Group ride\n",
      "   • 99: Unknown\n",
      "\n",
      "4. Payment Type Mapping: 7 payment types\n",
      "   • 0: Flex Fare\n",
      "   • 1: Credit card\n",
      "   • 2: Cash\n",
      "   • 3: No charge\n",
      "   • 4: Dispute\n",
      "   • 5: Unknown\n",
      "   • 6: Voided trip\n",
      "\n",
      "================================================================================\n",
      "✓ All lookup data ready for enrichment\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load Enrichment Lookup Data\n",
    "import csv\n",
    "\n",
    "# 1. Load Taxi Zone Lookup Data\n",
    "taxi_zone_lookup_path = \"/home/ubuntu/dat535-2025-group10/Metadata/taxi_zone_lookup.csv\"\n",
    "taxi_zones = {}\n",
    "\n",
    "with open(taxi_zone_lookup_path, 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        location_id = int(row['LocationID'])\n",
    "        taxi_zones[location_id] = {\n",
    "            'borough': row['Borough'],\n",
    "            'zone': row['Zone'],\n",
    "            'service_zone': row['service_zone']\n",
    "        }\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ENRICHMENT LOOKUP DATA LOADED\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(f\"1. Taxi Zone Lookup: {len(taxi_zones)} zones loaded\")\n",
    "print(\"   Sample zones:\")\n",
    "for zone_id in [1, 132, 138, 264, 265]:\n",
    "    if zone_id in taxi_zones:\n",
    "        info = taxi_zones[zone_id]\n",
    "        print(f\"   • ID {zone_id}: {info['zone']}, {info['borough']} ({info['service_zone']})\")\n",
    "print()\n",
    "\n",
    "# 2. Vendor ID Mapping (from TLC data dictionary)\n",
    "vendor_mapping = {\n",
    "    1: \"Creative Mobile Technologies\",\n",
    "    2: \"Curb Mobility\",\n",
    "    6: \"Myle Technologies\",\n",
    "    7: \"Helix\"\n",
    "}\n",
    "print(f\"2. Vendor Mapping: {len(vendor_mapping)} vendors\")\n",
    "for vid, name in vendor_mapping.items():\n",
    "    print(f\"   • {vid}: {name}\")\n",
    "print()\n",
    "\n",
    "# 3. Rate Code Mapping (from TLC data dictionary)\n",
    "ratecode_mapping = {\n",
    "    1: \"Standard rate\",\n",
    "    2: \"JFK\",\n",
    "    3: \"Newark\",\n",
    "    4: \"Nassau or Westchester\",\n",
    "    5: \"Negotiated fare\",\n",
    "    6: \"Group ride\",\n",
    "    99: \"Unknown\"\n",
    "}\n",
    "print(f\"3. Rate Code Mapping: {len(ratecode_mapping)} rate codes\")\n",
    "for rid, desc in ratecode_mapping.items():\n",
    "    print(f\"   • {rid}: {desc}\")\n",
    "print()\n",
    "\n",
    "# 4. Payment Type Mapping (from TLC data dictionary)\n",
    "payment_mapping = {\n",
    "    0: \"Flex Fare\",\n",
    "    1: \"Credit card\",\n",
    "    2: \"Cash\",\n",
    "    3: \"No charge\",\n",
    "    4: \"Dispute\",\n",
    "    5: \"Unknown\",\n",
    "    6: \"Voided trip\"\n",
    "}\n",
    "print(f\"4. Payment Type Mapping: {len(payment_mapping)} payment types\")\n",
    "for pid, method in payment_mapping.items():\n",
    "    print(f\"   • {pid}: {method}\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"✓ All lookup data ready for enrichment\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d34189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA ENRICHMENT FUNCTION DEFINED\n",
      "================================================================================\n",
      "\n",
      "Function: enrich_record(record)\n",
      "  • Type: MAP operation\n",
      "  • Input: Normalized record dictionary\n",
      "  • Output: Record with 9 additional enrichment fields\n",
      "\n",
      "Enrichment Fields Added:\n",
      "--------------------------------------------------------------------------------\n",
      "  Location Enrichment (6 fields):\n",
      "    • pickup_borough, pickup_zone, pickup_service_zone\n",
      "    • dropoff_borough, dropoff_zone, dropoff_service_zone\n",
      "\n",
      "  ID Enrichment (3 fields):\n",
      "    • vendor_name (from vendorid)\n",
      "    • rate_description (from ratecodeid)\n",
      "    • payment_method (from payment_type)\n",
      "\n",
      "Strategy:\n",
      "  • Keep original ID fields (needed for joins)\n",
      "  • Add human-readable descriptions alongside IDs\n",
      "  • Handle missing/unknown IDs gracefully with 'Unknown'\n",
      "\n",
      "================================================================================\n",
      "Total Silver Schema: 19 original + 9 enrichment + 5 Bronze + 4 Silver = 37 fields\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define Data Enrichment Function\n",
    "\n",
    "def enrich_record(record):\n",
    "    \"\"\"\n",
    "    MAP operation: Enrich record with human-readable descriptions for ID fields.\n",
    "    \n",
    "    Adds 9 new fields:\n",
    "    - pickup_borough, pickup_zone, pickup_service_zone\n",
    "    - dropoff_borough, dropoff_zone, dropoff_service_zone\n",
    "    - vendor_name\n",
    "    - rate_description\n",
    "    - payment_method\n",
    "    \n",
    "    Keeps original ID fields for joins in Gold layer.\n",
    "    \"\"\"\n",
    "    enriched = record.copy()\n",
    "    \n",
    "    # Enrich Pickup Location\n",
    "    pu_location_id = record.get('pulocationid')\n",
    "    if pu_location_id and pu_location_id in taxi_zones:\n",
    "        zone_info = taxi_zones[pu_location_id]\n",
    "        enriched['pickup_borough'] = zone_info['borough']\n",
    "        enriched['pickup_zone'] = zone_info['zone']\n",
    "        enriched['pickup_service_zone'] = zone_info['service_zone']\n",
    "    else:\n",
    "        enriched['pickup_borough'] = 'Unknown'\n",
    "        enriched['pickup_zone'] = 'Unknown'\n",
    "        enriched['pickup_service_zone'] = 'Unknown'\n",
    "    \n",
    "    # Enrich Dropoff Location\n",
    "    do_location_id = record.get('dolocationid')\n",
    "    if do_location_id and do_location_id in taxi_zones:\n",
    "        zone_info = taxi_zones[do_location_id]\n",
    "        enriched['dropoff_borough'] = zone_info['borough']\n",
    "        enriched['dropoff_zone'] = zone_info['zone']\n",
    "        enriched['dropoff_service_zone'] = zone_info['service_zone']\n",
    "    else:\n",
    "        enriched['dropoff_borough'] = 'Unknown'\n",
    "        enriched['dropoff_zone'] = 'Unknown'\n",
    "        enriched['dropoff_service_zone'] = 'Unknown'\n",
    "    \n",
    "    # Enrich Vendor ID\n",
    "    vendor_id = record.get('vendorid')\n",
    "    enriched['vendor_name'] = vendor_mapping.get(vendor_id, 'Unknown')\n",
    "    \n",
    "    # Enrich Rate Code\n",
    "    rate_code = record.get('ratecodeid')\n",
    "    if rate_code is not None:\n",
    "        enriched['rate_description'] = ratecode_mapping.get(int(rate_code), 'Unknown')\n",
    "    else:\n",
    "        enriched['rate_description'] = 'Unknown'\n",
    "    \n",
    "    # Enrich Payment Type\n",
    "    payment_type = record.get('payment_type')\n",
    "    if payment_type is not None:\n",
    "        enriched['payment_method'] = payment_mapping.get(int(payment_type), 'Unknown')\n",
    "    else:\n",
    "        enriched['payment_method'] = 'Unknown'\n",
    "    \n",
    "    return enriched\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA ENRICHMENT FUNCTION DEFINED\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"Function: enrich_record(record)\")\n",
    "print(\"  • Type: MAP operation\")\n",
    "print(\"  • Input: Normalized record dictionary\")\n",
    "print(\"  • Output: Record with 9 additional enrichment fields\")\n",
    "print()\n",
    "print(\"Enrichment Fields Added:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"  Location Enrichment (6 fields):\")\n",
    "print(\"    • pickup_borough, pickup_zone, pickup_service_zone\")\n",
    "print(\"    • dropoff_borough, dropoff_zone, dropoff_service_zone\")\n",
    "print()\n",
    "print(\"  ID Enrichment (3 fields):\")\n",
    "print(\"    • vendor_name (from vendorid)\")\n",
    "print(\"    • rate_description (from ratecodeid)\")\n",
    "print(\"    • payment_method (from payment_type)\")\n",
    "print()\n",
    "print(\"Strategy:\")\n",
    "print(\"  • Keep original ID fields (needed for joins)\")\n",
    "print(\"  • Add human-readable descriptions alongside IDs\")\n",
    "print(\"  • Handle missing/unknown IDs gracefully with 'Unknown'\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"Total Silver Schema: 19 original + 9 enrichment + 5 Bronze + 4 Silver = 37 fields\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434aded9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING ENRICHMENT FUNCTION\n",
      "================================================================================\n",
      "\n",
      "Sample Record 1:\n",
      "--------------------------------------------------------------------------------\n",
      "  Vendor: ID 2 → Curb Mobility\n",
      "  Rate Code: ID 1.0 → Standard rate\n",
      "  Payment: ID 1 → Credit card\n",
      "\n",
      "  Pickup: ID 48\n",
      "    → Clinton East, Manhattan\n",
      "    → Service Zone: Yellow Zone\n",
      "\n",
      "  Dropoff: ID 68\n",
      "    → East Chelsea, Manhattan\n",
      "    → Service Zone: Yellow Zone\n",
      "\n",
      "Sample Record 2:\n",
      "--------------------------------------------------------------------------------\n",
      "  Vendor: ID 2 → Curb Mobility\n",
      "  Rate Code: ID 1.0 → Standard rate\n",
      "  Payment: ID 1 → Credit card\n",
      "\n",
      "  Pickup: ID 231\n",
      "    → TriBeCa/Civic Center, Manhattan\n",
      "    → Service Zone: Yellow Zone\n",
      "\n",
      "  Dropoff: ID 164\n",
      "    → Midtown South, Manhattan\n",
      "    → Service Zone: Yellow Zone\n",
      "\n",
      "Sample Record 3:\n",
      "--------------------------------------------------------------------------------\n",
      "  Vendor: ID 2 → Curb Mobility\n",
      "  Rate Code: ID 1.0 → Standard rate\n",
      "  Payment: ID 1 → Credit card\n",
      "\n",
      "  Pickup: ID 90\n",
      "    → Flatiron, Manhattan\n",
      "    → Service Zone: Yellow Zone\n",
      "\n",
      "  Dropoff: ID 233\n",
      "    → UN/Turtle Bay South, Manhattan\n",
      "    → Service Zone: Yellow Zone\n",
      "\n",
      "================================================================================\n",
      "✓ Enrichment function working correctly!\n",
      "✓ Ready to integrate into batch processing pipeline\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Test Enrichment Function on Sample Data\n",
    "test_file = \"/home/ubuntu/dat535-2025-group10/bronze_layer/yellow_tripdata_2024-01_bronze\"\n",
    "df_test = spark.read.parquet(test_file)\n",
    "rdd_test = df_test.rdd.map(lambda row: row.asDict()).take(3)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TESTING ENRICHMENT FUNCTION\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "for i, record in enumerate(rdd_test, 1):\n",
    "    # Normalize first\n",
    "    normalized = normalize_record(record)\n",
    "    \n",
    "    # Then enrich\n",
    "    enriched = enrich_record(normalized)\n",
    "    \n",
    "    print(f\"Sample Record {i}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Vendor: ID {enriched.get('vendorid')} → {enriched.get('vendor_name')}\")\n",
    "    print(f\"  Rate Code: ID {enriched.get('ratecodeid')} → {enriched.get('rate_description')}\")\n",
    "    print(f\"  Payment: ID {enriched.get('payment_type')} → {enriched.get('payment_method')}\")\n",
    "    print()\n",
    "    print(f\"  Pickup: ID {enriched.get('pulocationid')}\")\n",
    "    print(f\"    → {enriched.get('pickup_zone')}, {enriched.get('pickup_borough')}\")\n",
    "    print(f\"    → Service Zone: {enriched.get('pickup_service_zone')}\")\n",
    "    print()\n",
    "    print(f\"  Dropoff: ID {enriched.get('dolocationid')}\")\n",
    "    print(f\"    → {enriched.get('dropoff_zone')}, {enriched.get('dropoff_borough')}\")\n",
    "    print(f\"    → Service Zone: {enriched.get('dropoff_service_zone')}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"✓ Enrichment function working correctly!\")\n",
    "print(\"✓ Ready to integrate into batch processing pipeline\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9401801",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Batch Processing with Enrichment\n",
    "\n",
    "Now we'll reprocess all 24 Bronze files with the complete Silver pipeline:\n",
    "\n",
    "**Complete RDD Pipeline:**\n",
    "1. Load Bronze Parquet → RDD of dictionaries\n",
    "2. **MAP:** `normalize_record()` - Schema standardization\n",
    "3. **MAP:** `enrich_record()` - Add human-readable fields ← NEW\n",
    "4. **MAP:** `validate_record_quality()` - Quality checks\n",
    "5. **FILTER:** `!should_remove_record()` - Remove critical issues\n",
    "6. **MAP:** `create_silver_record()` - Add Silver metadata\n",
    "7. Convert to DataFrame and write Parquet\n",
    "\n",
    "**Updated Schema:** 37 fields total\n",
    "- 19 original taxi fields\n",
    "- 9 enrichment fields (locations, vendor, rate, payment)\n",
    "- 5 Bronze metadata fields\n",
    "- 4 Silver metadata fields\n",
    "\n",
    "This follows best practice: Silver = cleaned + enriched data ready for analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f66244f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using existing Silver layer directory: /home/ubuntu/project2/silver_layer\n",
      "\n",
      "================================================================================\n",
      "BATCH PROCESSING - ALL 24 FILES\n",
      "================================================================================\n",
      "Source: /home/ubuntu/project2/bronze_layer\n",
      "Target: /home/ubuntu/project2/silver_layer\n",
      "\n",
      "Files to process: 24\n",
      "\n",
      "Processing files (RDD MapReduce pipeline):\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/22 10:22:02 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/11/22 10:22:02 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/24] yellow_tripdata_2023-01_bronze      | In: 3,066,766 | Out: 2,970,852 | Removed:  3.13% | Clean: 99.24% | 225.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2/24] yellow_tripdata_2023-02_bronze      | In: 2,913,955 | Out: 2,826,497 | Removed:  3.00% | Clean: 99.22% | 212.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3/24] yellow_tripdata_2023-03_bronze      | In: 3,403,766 | Out: 3,297,701 | Removed:  3.12% | Clean: 99.19% | 247.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4/24] yellow_tripdata_2023-04_bronze      | In: 3,288,250 | Out: 3,191,927 | Removed:  2.93% | Clean: 99.15% | 239.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5/24] yellow_tripdata_2023-05_bronze      | In: 3,513,649 | Out: 3,408,275 | Removed:  3.00% | Clean: 99.15% | 254.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6/24] yellow_tripdata_2023-06_bronze      | In: 3,307,234 | Out: 3,206,460 | Removed:  3.05% | Clean: 99.11% | 241.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7/24] yellow_tripdata_2023-07_bronze      | In: 2,907,108 | Out: 2,814,722 | Removed:  3.18% | Clean: 98.99% | 210.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8/24] yellow_tripdata_2023-08_bronze      | In: 2,824,209 | Out: 2,726,253 | Removed:  3.47% | Clean: 98.94% |  89.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9/24] yellow_tripdata_2023-09_bronze      | In: 2,846,722 | Out: 2,710,128 | Removed:  4.80% | Clean: 99.00% |  89.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/24] yellow_tripdata_2023-10_bronze      | In: 3,522,285 | Out: 3,355,217 | Removed:  4.74% | Clean: 98.99% | 100.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/24] yellow_tripdata_2023-11_bronze      | In: 3,339,715 | Out: 3,195,926 | Removed:  4.31% | Clean: 98.95% |  96.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/24] yellow_tripdata_2023-12_bronze      | In: 3,376,567 | Out: 3,261,785 | Removed:  3.40% | Clean: 98.79% |  96.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/24] yellow_tripdata_2024-01_bronze      | In: 2,964,624 | Out: 2,873,462 | Removed:  3.07% | Clean: 98.80% |  90.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/24] yellow_tripdata_2024-02_bronze      | In: 3,007,526 | Out: 2,906,313 | Removed:  3.37% | Clean: 98.70% |  90.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/24] yellow_tripdata_2024-03_bronze      | In: 3,582,628 | Out: 3,456,528 | Removed:  3.52% | Clean: 98.40% | 103.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/24] yellow_tripdata_2024-04_bronze      | In: 3,514,289 | Out: 3,430,395 | Removed:  2.39% | Clean: 98.42% | 101.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/24] yellow_tripdata_2024-05_bronze      | In: 3,723,833 | Out: 3,636,065 | Removed:  2.36% | Clean: 98.41% | 107.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/24] yellow_tripdata_2024-06_bronze      | In: 3,539,193 | Out: 3,452,076 | Removed:  2.46% | Clean: 98.32% | 103.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/24] yellow_tripdata_2024-07_bronze      | In: 3,076,903 | Out: 3,001,186 | Removed:  2.46% | Clean: 98.17% |  90.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/24] yellow_tripdata_2024-08_bronze      | In: 2,979,183 | Out: 2,896,886 | Removed:  2.76% | Clean: 98.12% |  90.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/24] yellow_tripdata_2024-09_bronze      | In: 3,633,030 | Out: 3,523,197 | Removed:  3.02% | Clean: 98.07% | 105.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22/24] yellow_tripdata_2024-10_bronze      | In: 3,833,771 | Out: 3,717,924 | Removed:  3.02% | Clean: 98.11% | 110.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/24] yellow_tripdata_2024-11_bronze      | In: 3,646,369 | Out: 3,549,715 | Removed:  2.65% | Clean: 98.11% | 102.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 146:==========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/24] yellow_tripdata_2024-12_bronze      | In: 3,668,371 | Out: 3,563,400 | Removed:  2.86% | Clean: 97.93% | 102.1s\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "BATCH PROCESSING COMPLETE!\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Batch Processing: Clean All 24 Files\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, TimestampType\n",
    "\n",
    "bronze_path = \"/home/ubuntu/dat535-2025-group10/bronze_layer\"\n",
    "silver_output_path = \"/home/ubuntu/dat535-2025-group10/silver_layer\"\n",
    "\n",
    "# Create output directory\n",
    "if not os.path.exists(silver_output_path):\n",
    "    os.makedirs(silver_output_path)\n",
    "    print(f\"✓ Created Silver layer output directory: {silver_output_path}\")\n",
    "else:\n",
    "    print(f\"✓ Using existing Silver layer directory: {silver_output_path}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"BATCH PROCESSING - ALL 24 FILES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Source: {bronze_path}\")\n",
    "print(f\"Target: {silver_output_path}\")\n",
    "print()\n",
    "\n",
    "# Get all Bronze files\n",
    "bronze_dirs = sorted([d for d in os.listdir(bronze_path) if os.path.isdir(os.path.join(bronze_path, d))])\n",
    "print(f\"Files to process: {len(bronze_dirs)}\")\n",
    "print()\n",
    "\n",
    "# Define explicit schema for Silver DataFrame (37 fields: 19 original + 9 enrichment + 5 Bronze + 4 Silver)\n",
    "silver_schema = StructType([\n",
    "    # Original taxi fields (19)\n",
    "    StructField(\"vendorid\", LongType(), True),\n",
    "    StructField(\"tpep_pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"tpep_dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"passenger_count\", DoubleType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"ratecodeid\", DoubleType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"pulocationid\", LongType(), True),\n",
    "    StructField(\"dolocationid\", LongType(), True),\n",
    "    StructField(\"payment_type\", LongType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"extra\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True),\n",
    "    StructField(\"improvement_surcharge\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"congestion_surcharge\", DoubleType(), True),\n",
    "    StructField(\"airport_fee\", DoubleType(), True),\n",
    "    # Enrichment fields (9)\n",
    "    StructField(\"pickup_borough\", StringType(), True),\n",
    "    StructField(\"pickup_zone\", StringType(), True),\n",
    "    StructField(\"pickup_service_zone\", StringType(), True),\n",
    "    StructField(\"dropoff_borough\", StringType(), True),\n",
    "    StructField(\"dropoff_zone\", StringType(), True),\n",
    "    StructField(\"dropoff_service_zone\", StringType(), True),\n",
    "    StructField(\"vendor_name\", StringType(), True),\n",
    "    StructField(\"rate_description\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    # Bronze metadata (5)\n",
    "    StructField(\"_bronze_ingestion_timestamp\", StringType(), True),\n",
    "    StructField(\"_bronze_source_file\", StringType(), True),\n",
    "    StructField(\"_bronze_record_id\", StringType(), True),\n",
    "    StructField(\"_bronze_status\", StringType(), True),\n",
    "    StructField(\"_bronze_quality_flags\", StringType(), True),\n",
    "    # Silver metadata (4)\n",
    "    StructField(\"_silver_cleaning_timestamp\", StringType(), True),\n",
    "    StructField(\"_silver_quality_issues\", StringType(), True),\n",
    "    StructField(\"_silver_status\", StringType(), True),\n",
    "    StructField(\"_silver_issue_count\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Track overall statistics\n",
    "overall_stats = {\n",
    "    'files_processed': 0,\n",
    "    'total_input_records': 0,\n",
    "    'total_removed_records': 0,\n",
    "    'total_output_records': 0,\n",
    "    'total_clean_records': 0,\n",
    "    'total_flagged_records': 0,\n",
    "    'processing_times': [],\n",
    "    'file_stats': []\n",
    "}\n",
    "\n",
    "print(\"Processing files (RDD MapReduce pipeline):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Process each file\n",
    "for file_idx, bronze_dir in enumerate(bronze_dirs, 1):\n",
    "    file_start_time = time.time()\n",
    "    \n",
    "    # Load Bronze file\n",
    "    bronze_file_path = os.path.join(bronze_path, bronze_dir)\n",
    "    df_bronze = spark.read.parquet(bronze_file_path)\n",
    "    \n",
    "    # RDD Processing Pipeline (all MapReduce operations)\n",
    "    rdd_bronze = df_bronze.rdd.map(lambda row: row.asDict())\n",
    "    rdd_normalized = rdd_bronze.map(normalize_record)\n",
    "    rdd_enriched = rdd_normalized.map(enrich_record)  # NEW: Add enrichment\n",
    "    rdd_validated = rdd_enriched.map(validate_record_quality)\n",
    "    rdd_filtered = rdd_validated.filter(lambda x: not should_remove_record(x[1]))\n",
    "    rdd_silver = rdd_filtered.map(create_silver_record)\n",
    "    \n",
    "    # Count records (trigger computation)\n",
    "    input_count = rdd_bronze.count()\n",
    "    output_count = rdd_silver.count()\n",
    "    removed_count = input_count - output_count\n",
    "    \n",
    "    # Get status distribution using REDUCEBYKEY\n",
    "    status_counts = rdd_silver \\\n",
    "        .map(lambda rec: (rec['_silver_status'], 1)) \\\n",
    "        .reduceByKey(lambda a, b: a + b) \\\n",
    "        .collectAsMap()\n",
    "    \n",
    "    clean_count = status_counts.get('clean', 0)\n",
    "    flagged_count = status_counts.get('flagged', 0)\n",
    "    \n",
    "    # Convert dictionary RDD to Row RDD, then to DataFrame with explicit schema\n",
    "    # MAP operation: Convert dict to tuple in schema field order (37 fields)\n",
    "    def dict_to_tuple(d):\n",
    "        return (\n",
    "            # Original 19 fields\n",
    "            d.get('vendorid'), d.get('tpep_pickup_datetime'), d.get('tpep_dropoff_datetime'),\n",
    "            d.get('passenger_count'), d.get('trip_distance'), d.get('ratecodeid'),\n",
    "            d.get('store_and_fwd_flag'), d.get('pulocationid'), d.get('dolocationid'),\n",
    "            d.get('payment_type'), d.get('fare_amount'), d.get('extra'),\n",
    "            d.get('mta_tax'), d.get('tip_amount'), d.get('tolls_amount'),\n",
    "            d.get('improvement_surcharge'), d.get('total_amount'), d.get('congestion_surcharge'),\n",
    "            d.get('airport_fee'),\n",
    "            # Enrichment 9 fields\n",
    "            d.get('pickup_borough'), d.get('pickup_zone'), d.get('pickup_service_zone'),\n",
    "            d.get('dropoff_borough'), d.get('dropoff_zone'), d.get('dropoff_service_zone'),\n",
    "            d.get('vendor_name'), d.get('rate_description'), d.get('payment_method'),\n",
    "            # Bronze 5 fields\n",
    "            d.get('_bronze_ingestion_timestamp'), d.get('_bronze_source_file'),\n",
    "            d.get('_bronze_record_id'), d.get('_bronze_status'), d.get('_bronze_quality_flags'),\n",
    "            # Silver 4 fields\n",
    "            d.get('_silver_cleaning_timestamp'), d.get('_silver_quality_issues'),\n",
    "            d.get('_silver_status'), d.get('_silver_issue_count')\n",
    "        )\n",
    "    \n",
    "    rdd_tuples = rdd_silver.map(dict_to_tuple)\n",
    "    silver_df = spark.createDataFrame(rdd_tuples, schema=silver_schema)\n",
    "    \n",
    "    # Write to Silver layer\n",
    "    output_file_path = os.path.join(silver_output_path, bronze_dir.replace('_bronze', '_silver'))\n",
    "    silver_df.write.mode('overwrite').parquet(output_file_path)\n",
    "    \n",
    "    # Release memory\n",
    "    rdd_silver.unpersist()\n",
    "    del rdd_bronze, rdd_normalized, rdd_enriched, rdd_validated, rdd_filtered, rdd_silver, rdd_tuples, silver_df, df_bronze\n",
    "    \n",
    "    # Record statistics\n",
    "    file_time = time.time() - file_start_time\n",
    "    overall_stats['files_processed'] += 1\n",
    "    overall_stats['total_input_records'] += input_count\n",
    "    overall_stats['total_removed_records'] += removed_count\n",
    "    overall_stats['total_output_records'] += output_count\n",
    "    overall_stats['total_clean_records'] += clean_count\n",
    "    overall_stats['total_flagged_records'] += flagged_count\n",
    "    overall_stats['processing_times'].append(file_time)\n",
    "    \n",
    "    # Store per-file stats\n",
    "    overall_stats['file_stats'].append({\n",
    "        'file': bronze_dir,\n",
    "        'input': input_count,\n",
    "        'removed': removed_count,\n",
    "        'output': output_count,\n",
    "        'clean': clean_count,\n",
    "        'flagged': flagged_count,\n",
    "        'time': file_time\n",
    "    })\n",
    "    \n",
    "    # Progress output\n",
    "    removal_pct = (removed_count / input_count * 100) if input_count > 0 else 0\n",
    "    clean_pct = (clean_count / output_count * 100) if output_count > 0 else 0\n",
    "    \n",
    "    print(f\"[{file_idx:2d}/24] {bronze_dir:35s} | \"\n",
    "          f\"In: {input_count:7,} | Out: {output_count:7,} | \"\n",
    "          f\"Removed: {removal_pct:5.2f}% | Clean: {clean_pct:5.2f}% | \"\n",
    "          f\"{file_time:5.1f}s\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"BATCH PROCESSING COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c7ac79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENRICHED SILVER LAYER - FINAL SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      "OVERALL STATISTICS:\n",
      "--------------------------------------------------------------------------------\n",
      "Files Processed:           24/24\n",
      "Total Input Records:       79,479,946\n",
      "Total Records Removed:     2,507,056 (3.15%)\n",
      "Total Output Records:      76,972,890 (96.85%)\n",
      "\n",
      "SILVER LAYER QUALITY DISTRIBUTION:\n",
      "--------------------------------------------------------------------------------\n",
      "Clean Records:             75,938,992 (98.66%)\n",
      "Flagged Records:           1,033,898 (1.34%)\n",
      "\n",
      "PERFORMANCE METRICS:\n",
      "--------------------------------------------------------------------------------\n",
      "Total Processing Time:     3300.6 seconds (55.0 minutes)\n",
      "Average Time per File:     137.5 seconds\n",
      "Throughput:                24,080 records/second\n",
      "\n",
      "DATA QUALITY TRENDS BY TIME PERIOD:\n",
      "--------------------------------------------------------------------------------\n",
      "Period     | Files | Removal %  | Clean %   \n",
      "--------------------------------------------------------------------------------\n",
      "2023       |    12 |      3.51% |     99.06%\n",
      "2024       |    12 |      2.82% |     98.28%\n",
      "\n",
      "ENRICHED SILVER LAYER SCHEMA:\n",
      "--------------------------------------------------------------------------------\n",
      "Total Fields:              37\n",
      "  • Original Taxi Fields:  19\n",
      "  • Enrichment Fields:     9  ← NEW\n",
      "  • Bronze Metadata:       5\n",
      "  • Silver Metadata:       4\n",
      "\n",
      "Enrichment Fields Added:\n",
      "  Location Enrichment (6):\n",
      "    - pickup_borough, pickup_zone, pickup_service_zone\n",
      "    - dropoff_borough, dropoff_zone, dropoff_service_zone\n",
      "  ID Enrichment (3):\n",
      "    - vendor_name, rate_description, payment_method\n",
      "\n",
      "RDD MAPREDUCE OPERATIONS APPLIED:\n",
      "--------------------------------------------------------------------------------\n",
      "Per Record Transformations:\n",
      "  1. map(row.asDict) - Convert Spark Rows to dictionaries\n",
      "  2. map(normalize_record) - Schema standardization\n",
      "  3. map(enrich_record) - Add human-readable fields ← NEW\n",
      "  4. map(validate_record_quality) - Quality validation\n",
      "  5. filter(!should_remove_record) - Remove critical issues\n",
      "  6. map(create_silver_record) - Add Silver metadata\n",
      "  7. map(dict_to_tuple) - Convert to tuple for DataFrame\n",
      "\n",
      "Aggregation Operations:\n",
      "  • reduceByKey(sum) - Count records by status\n",
      "  • count() - Trigger RDD computation\n",
      "\n",
      "STORAGE INFORMATION:\n",
      "--------------------------------------------------------------------------------\n",
      "Bronze Layer Size:         2.43 GB\n",
      "Silver Layer Size:         3.04 GB\n",
      "Size Change:               +24.8%\n",
      "  (Size increase due to 9 enrichment + 4 Silver metadata fields)\n",
      "\n",
      "OUTPUT LOCATION:\n",
      "--------------------------------------------------------------------------------\n",
      "Silver Layer: /home/ubuntu/project2/silver_layer\n",
      "  • 24 monthly partitions\n",
      "  • Parquet format with Snappy compression\n",
      "  • 37 fields per record (enriched and cleaned)\n",
      "\n",
      "================================================================================\n",
      "✓ ENRICHED SILVER LAYER COMPLETE!\n",
      "✓ All processing done using RDD MapReduce operations only\n",
      "✓ Data is cleaned, enriched, and ready for Gold layer analytics\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate Summary Report with Enrichment\n",
    "print(\"=\" * 80)\n",
    "print(\"ENRICHED SILVER LAYER - FINAL SUMMARY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"OVERALL STATISTICS:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Files Processed:           {overall_stats['files_processed']}/24\")\n",
    "print(f\"Total Input Records:       {overall_stats['total_input_records']:,}\")\n",
    "print(f\"Total Records Removed:     {overall_stats['total_removed_records']:,} \"\n",
    "      f\"({overall_stats['total_removed_records']/overall_stats['total_input_records']*100:.2f}%)\")\n",
    "print(f\"Total Output Records:      {overall_stats['total_output_records']:,} \"\n",
    "      f\"({overall_stats['total_output_records']/overall_stats['total_input_records']*100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "print(\"SILVER LAYER QUALITY DISTRIBUTION:\")\n",
    "print(\"-\" * 80)\n",
    "clean_pct = (overall_stats['total_clean_records']/overall_stats['total_output_records']*100)\n",
    "flagged_pct = (overall_stats['total_flagged_records']/overall_stats['total_output_records']*100)\n",
    "print(f\"Clean Records:             {overall_stats['total_clean_records']:,} ({clean_pct:.2f}%)\")\n",
    "print(f\"Flagged Records:           {overall_stats['total_flagged_records']:,} ({flagged_pct:.2f}%)\")\n",
    "print()\n",
    "\n",
    "print(\"PERFORMANCE METRICS:\")\n",
    "print(\"-\" * 80)\n",
    "total_time = sum(overall_stats['processing_times'])\n",
    "avg_time = total_time / len(overall_stats['processing_times'])\n",
    "throughput = overall_stats['total_input_records'] / total_time\n",
    "\n",
    "print(f\"Total Processing Time:     {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
    "print(f\"Average Time per File:     {avg_time:.1f} seconds\")\n",
    "print(f\"Throughput:                {throughput:,.0f} records/second\")\n",
    "print()\n",
    "\n",
    "print(\"DATA QUALITY TRENDS BY TIME PERIOD:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Period':<10} | {'Files':<5} | {'Removal %':<10} | {'Clean %':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Group by year\n",
    "stats_2023 = [s for s in overall_stats['file_stats'] if '2023' in s['file']]\n",
    "stats_2024 = [s for s in overall_stats['file_stats'] if '2024' in s['file']]\n",
    "\n",
    "for period, stats in [('2023', stats_2023), ('2024', stats_2024)]:\n",
    "    total_in = sum(s['input'] for s in stats)\n",
    "    total_removed = sum(s['removed'] for s in stats)\n",
    "    total_out = sum(s['output'] for s in stats)\n",
    "    total_clean = sum(s['clean'] for s in stats)\n",
    "    \n",
    "    removal_pct = (total_removed / total_in * 100) if total_in > 0 else 0\n",
    "    clean_pct = (total_clean / total_out * 100) if total_out > 0 else 0\n",
    "    \n",
    "    print(f\"{period:<10} | {len(stats):>5} | {removal_pct:>9.2f}% | {clean_pct:>9.2f}%\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"ENRICHED SILVER LAYER SCHEMA:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Total Fields:              37\")\n",
    "print(f\"  • Original Taxi Fields:  19\")\n",
    "print(f\"  • Enrichment Fields:     9  ← NEW\")\n",
    "print(f\"  • Bronze Metadata:       5\")\n",
    "print(f\"  • Silver Metadata:       4\")\n",
    "print()\n",
    "print(\"Enrichment Fields Added:\")\n",
    "print(\"  Location Enrichment (6):\")\n",
    "print(\"    - pickup_borough, pickup_zone, pickup_service_zone\")\n",
    "print(\"    - dropoff_borough, dropoff_zone, dropoff_service_zone\")\n",
    "print(\"  ID Enrichment (3):\")\n",
    "print(\"    - vendor_name, rate_description, payment_method\")\n",
    "print()\n",
    "\n",
    "print(\"RDD MAPREDUCE OPERATIONS APPLIED:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Per Record Transformations:\")\n",
    "print(\"  1. map(row.asDict) - Convert Spark Rows to dictionaries\")\n",
    "print(\"  2. map(normalize_record) - Schema standardization\")\n",
    "print(\"  3. map(enrich_record) - Add human-readable fields ← NEW\")\n",
    "print(\"  4. map(validate_record_quality) - Quality validation\")\n",
    "print(\"  5. filter(!should_remove_record) - Remove critical issues\")\n",
    "print(\"  6. map(create_silver_record) - Add Silver metadata\")\n",
    "print(\"  7. map(dict_to_tuple) - Convert to tuple for DataFrame\")\n",
    "print()\n",
    "print(\"Aggregation Operations:\")\n",
    "print(\"  • reduceByKey(sum) - Count records by status\")\n",
    "print(\"  • count() - Trigger RDD computation\")\n",
    "print()\n",
    "\n",
    "print(\"STORAGE INFORMATION:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calculate directory sizes\n",
    "bronze_size = sum(os.path.getsize(os.path.join(bronze_path, d, f)) \n",
    "                 for d in bronze_dirs \n",
    "                 for f in os.listdir(os.path.join(bronze_path, d)) \n",
    "                 if os.path.isfile(os.path.join(bronze_path, d, f)))\n",
    "\n",
    "silver_dirs = [d.replace('_bronze', '_silver') for d in bronze_dirs]\n",
    "silver_size = sum(os.path.getsize(os.path.join(silver_output_path, d, f)) \n",
    "                 for d in silver_dirs \n",
    "                 for f in os.listdir(os.path.join(silver_output_path, d)) \n",
    "                 if os.path.isfile(os.path.join(silver_output_path, d, f)))\n",
    "\n",
    "print(f\"Bronze Layer Size:         {bronze_size/(1024**3):.2f} GB\")\n",
    "print(f\"Silver Layer Size:         {silver_size/(1024**3):.2f} GB\")\n",
    "print(f\"Size Change:               {(silver_size/bronze_size-1)*100:+.1f}%\")\n",
    "print(f\"  (Size increase due to 9 enrichment + 4 Silver metadata fields)\")\n",
    "print()\n",
    "\n",
    "print(\"OUTPUT LOCATION:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Silver Layer: {silver_output_path}\")\n",
    "print(f\"  • 24 monthly partitions\")\n",
    "print(f\"  • Parquet format with Snappy compression\")\n",
    "print(f\"  • 37 fields per record (enriched and cleaned)\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"✓ ENRICHED SILVER LAYER COMPLETE!\")\n",
    "print(\"✓ All processing done using RDD MapReduce operations only\")\n",
    "print(\"✓ Data is cleaned, enriched, and ready for Gold layer analytics\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08eb6c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENRICHED SILVER LAYER OUTPUT VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "Sample File: /home/ubuntu/project2/silver_layer/yellow_tripdata_2024-12_silver\n",
      "Record Count: 3,563,400\n",
      "\n",
      "Schema (37 fields total):\n",
      "--------------------------------------------------------------------------------\n",
      " 1. vendorid                            Long            [Original]\n",
      " 2. tpep_pickup_datetime                Timestamp       [Original]\n",
      " 3. tpep_dropoff_datetime               Timestamp       [Original]\n",
      " 4. passenger_count                     Double          [Original]\n",
      " 5. trip_distance                       Double          [Original]\n",
      " 6. ratecodeid                          Double          [Original]\n",
      " 7. store_and_fwd_flag                  String          [Original]\n",
      " 8. pulocationid                        Long            [Original]\n",
      " 9. dolocationid                        Long            [Original]\n",
      "10. payment_type                        Long            [Original]\n",
      "11. fare_amount                         Double          [Original]\n",
      "12. extra                               Double          [Original]\n",
      "13. mta_tax                             Double          [Original]\n",
      "14. tip_amount                          Double          [Original]\n",
      "15. tolls_amount                        Double          [Original]\n",
      "16. improvement_surcharge               Double          [Original]\n",
      "17. total_amount                        Double          [Original]\n",
      "18. congestion_surcharge                Double          [Original]\n",
      "19. airport_fee                         Double          [Original]\n",
      "20. pickup_borough                      String          [Enrichment]\n",
      "21. pickup_zone                         String          [Enrichment]\n",
      "22. pickup_service_zone                 String          [Enrichment]\n",
      "23. dropoff_borough                     String          [Enrichment]\n",
      "24. dropoff_zone                        String          [Enrichment]\n",
      "25. dropoff_service_zone                String          [Enrichment]\n",
      "26. vendor_name                         String          [Enrichment]\n",
      "27. rate_description                    String          [Enrichment]\n",
      "28. payment_method                      String          [Enrichment]\n",
      "29. _bronze_ingestion_timestamp         String          [Bronze Meta]\n",
      "30. _bronze_source_file                 String          [Bronze Meta]\n",
      "31. _bronze_record_id                   String          [Bronze Meta]\n",
      "32. _bronze_status                      String          [Bronze Meta]\n",
      "33. _bronze_quality_flags               String          [Bronze Meta]\n",
      "34. _silver_cleaning_timestamp          String          [Silver Meta]\n",
      "35. _silver_quality_issues              String          [Silver Meta]\n",
      "36. _silver_status                      String          [Silver Meta]\n",
      "37. _silver_issue_count                 Long            [Silver Meta]\n",
      "\n",
      "Sample Enriched Records:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✓ CLEAN RECORD (showing enrichment):\n",
      "-RECORD 0-----------------------------------\n",
      " tpep_pickup_datetime | 2024-12-29 14:34:09 \n",
      " pulocationid         | 45                  \n",
      " pickup_zone          | Chinatown           \n",
      " pickup_borough       | Manhattan           \n",
      " dolocationid         | 162                 \n",
      " dropoff_zone         | Midtown East        \n",
      " dropoff_borough      | Manhattan           \n",
      " vendorid             | 2                   \n",
      " vendor_name          | Curb Mobility       \n",
      " payment_type         | 2                   \n",
      " payment_method       | Cash                \n",
      " fare_amount          | 26.1                \n",
      " total_amount         | 30.1                \n",
      " _silver_status       | clean               \n",
      "\n",
      "\n",
      "================================================================================\n",
      "✓ Enriched Silver Layer Verification Complete!\n",
      "✓ All ID fields now have human-readable descriptions\n",
      "✓ Ready for Gold layer analytics with enriched data\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify Enriched Silver Layer Output\n",
    "print(\"=\" * 80)\n",
    "print(\"ENRICHED SILVER LAYER OUTPUT VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Load a sample Silver file\n",
    "sample_silver_path = f\"{silver_output_path}/yellow_tripdata_2024-12_silver\"\n",
    "sample_silver_df = spark.read.parquet(sample_silver_path)\n",
    "\n",
    "print(f\"Sample File: {sample_silver_path}\")\n",
    "print(f\"Record Count: {sample_silver_df.count():,}\")\n",
    "print()\n",
    "\n",
    "print(\"Schema (37 fields total):\")\n",
    "print(\"-\" * 80)\n",
    "for i, field in enumerate(sample_silver_df.schema.fields, 1):\n",
    "    field_type = str(field.dataType).replace('Type', '').replace('()', '')\n",
    "    category = \"\"\n",
    "    if i <= 19:\n",
    "        category = \"Original\"\n",
    "    elif i <= 28:\n",
    "        category = \"Enrichment\"\n",
    "    elif i <= 33:\n",
    "        category = \"Bronze Meta\"\n",
    "    else:\n",
    "        category = \"Silver Meta\"\n",
    "    print(f\"{i:2}. {field.name:<35} {field_type:<15} [{category}]\")\n",
    "\n",
    "print()\n",
    "print(\"Sample Enriched Records:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Show sample clean record with enrichment\n",
    "clean_sample = sample_silver_df.filter(sample_silver_df._silver_status == 'clean').limit(1)\n",
    "print(\"\\n✓ CLEAN RECORD (showing enrichment):\")\n",
    "clean_sample.select(\n",
    "    'tpep_pickup_datetime',\n",
    "    'pulocationid', 'pickup_zone', 'pickup_borough',\n",
    "    'dolocationid', 'dropoff_zone', 'dropoff_borough',\n",
    "    'vendorid', 'vendor_name',\n",
    "    'payment_type', 'payment_method',\n",
    "    'fare_amount', 'total_amount',\n",
    "    '_silver_status'\n",
    ").show(1, truncate=False, vertical=True)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"✓ Enriched Silver Layer Verification Complete!\")\n",
    "print(\"✓ All ID fields now have human-readable descriptions\")\n",
    "print(\"✓ Ready for Gold layer analytics with enriched data\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f30052e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visual Verification: Sample Records from Different Files\n",
    "\n",
    "Let's examine sample records from different time periods to verify data quality and enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f52773c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "FILE: yellow_tripdata_2023-01_silver\n",
      "====================================================================================================\n",
      "Total Records: 2,970,852\n",
      "Total Fields: 37\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY TRIP FIELDS:\n",
      "----------------------------------------------------------------------------------------------------\n",
      " vendorid tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  pulocationid  dolocationid  ratecodeid  payment_type  fare_amount  total_amount\n",
      "        2  2023-01-01 00:32:10   2023-01-01 00:40:36              1.0           0.97           161           141         1.0             2          9.3         14.30\n",
      "        2  2023-01-01 00:55:08   2023-01-01 01:01:27              1.0           1.10            43           237         1.0             1          7.9         16.90\n",
      "        2  2023-01-01 00:25:04   2023-01-01 00:37:49              1.0           2.51            48           238         1.0             1         14.9         34.90\n",
      "        2  2023-01-01 00:10:29   2023-01-01 00:21:19              1.0           1.43           107            79         1.0             1         11.4         19.68\n",
      "        2  2023-01-01 00:50:34   2023-01-01 01:02:52              1.0           1.84           161           137         1.0             1         12.8         27.80\n",
      "\n",
      "ENRICHMENT FIELDS:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  vendor_name pickup_borough    pickup_zone dropoff_borough          dropoff_zone rate_description payment_method\n",
      "Curb Mobility      Manhattan Midtown Center       Manhattan       Lenox Hill West    Standard rate           Cash\n",
      "Curb Mobility      Manhattan   Central Park       Manhattan Upper East Side South    Standard rate    Credit card\n",
      "Curb Mobility      Manhattan   Clinton East       Manhattan Upper West Side North    Standard rate    Credit card\n",
      "Curb Mobility      Manhattan       Gramercy       Manhattan          East Village    Standard rate    Credit card\n",
      "Curb Mobility      Manhattan Midtown Center       Manhattan              Kips Bay    Standard rate    Credit card\n",
      "\n",
      "SILVER METADATA:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "_silver_status  _silver_issue_count _silver_quality_issues\n",
      "         clean                    0                   None\n",
      "         clean                    0                   None\n",
      "         clean                    0                   None\n",
      "         clean                    0                   None\n",
      "         clean                    0                   None\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "FILE: yellow_tripdata_2023-06_silver\n",
      "====================================================================================================\n",
      "Total Records: 3,206,460\n",
      "Total Fields: 37\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY TRIP FIELDS:\n",
      "----------------------------------------------------------------------------------------------------\n",
      " vendorid tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  pulocationid  dolocationid  ratecodeid  payment_type  fare_amount  total_amount\n",
      "        1  2023-06-01 00:08:48   2023-06-01 00:29:41              1.0           3.40           140           238         1.0             1         21.9         33.60\n",
      "        1  2023-06-01 00:48:24   2023-06-01 01:07:07              1.0          10.20           138            97         1.0             1         40.8         60.05\n",
      "        2  2023-06-01 00:54:03   2023-06-01 01:17:29              3.0           9.83           100           244         1.0             1         39.4         53.28\n",
      "        2  2023-06-01 00:18:44   2023-06-01 00:27:18              1.0           1.17           137           234         1.0             1          9.3         15.02\n",
      "        1  2023-06-01 00:32:36   2023-06-01 00:45:52              2.0           3.60           249            33         1.0             1         18.4         28.05\n",
      "\n",
      "ENRICHMENT FIELDS:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "                 vendor_name pickup_borough       pickup_zone dropoff_borough             dropoff_zone rate_description payment_method\n",
      "Creative Mobile Technologies      Manhattan   Lenox Hill East       Manhattan    Upper West Side North    Standard rate    Credit card\n",
      "Creative Mobile Technologies         Queens LaGuardia Airport        Brooklyn              Fort Greene    Standard rate    Credit card\n",
      "               Curb Mobility      Manhattan  Garment District       Manhattan Washington Heights South    Standard rate    Credit card\n",
      "               Curb Mobility      Manhattan          Kips Bay       Manhattan                 Union Sq    Standard rate    Credit card\n",
      "Creative Mobile Technologies      Manhattan      West Village        Brooklyn         Brooklyn Heights    Standard rate    Credit card\n",
      "\n",
      "SILVER METADATA:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "_silver_status  _silver_issue_count _silver_quality_issues\n",
      "         clean                    0                   None\n",
      "         clean                    0                   None\n",
      "         clean                    0                   None\n",
      "         clean                    0                   None\n",
      "         clean                    0                   None\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "FILE: yellow_tripdata_2024-01_silver\n",
      "====================================================================================================\n",
      "Total Records: 2,873,462\n",
      "Total Fields: 37\n",
      "\n",
      "KEY TRIP FIELDS:\n",
      "----------------------------------------------------------------------------------------------------\n",
      " vendorid tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  pulocationid  dolocationid  ratecodeid  payment_type  fare_amount  total_amount\n",
      "        2  2024-01-13 03:18:09   2024-01-13 03:24:37              1.0           2.31            48            68         1.0             1         11.4         19.68\n",
      "        2  2024-01-13 03:52:58   2024-01-13 04:01:18              1.0           2.61           231           164         1.0             1         13.5         20.50\n",
      "        2  2024-01-13 03:26:02   2024-01-13 03:34:43              1.0           1.79            90           233         1.0             1         11.4         19.68\n",
      "        2  2024-01-13 03:53:44   2024-01-13 04:10:56              1.0           6.58           141           244         1.0             1         27.5         39.00\n",
      "        2  2024-01-13 02:58:28   2024-01-13 03:14:33              1.0           3.40            79           246         1.0             1         18.4         28.08\n",
      "\n",
      "ENRICHMENT FIELDS:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  vendor_name pickup_borough          pickup_zone dropoff_borough              dropoff_zone rate_description payment_method\n",
      "Curb Mobility      Manhattan         Clinton East       Manhattan              East Chelsea    Standard rate    Credit card\n",
      "Curb Mobility      Manhattan TriBeCa/Civic Center       Manhattan             Midtown South    Standard rate    Credit card\n",
      "Curb Mobility      Manhattan             Flatiron       Manhattan       UN/Turtle Bay South    Standard rate    Credit card\n",
      "Curb Mobility      Manhattan      Lenox Hill West       Manhattan  Washington Heights South    Standard rate    Credit card\n",
      "Curb Mobility      Manhattan         East Village       Manhattan West Chelsea/Hudson Yards    Standard rate    Credit card\n",
      "\n",
      "SILVER METADATA:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "_silver_status  _silver_issue_count _silver_quality_issues\n",
      "         clean                    0                   None\n",
      "         clean                    0                   None\n",
      "         clean                    0                   None\n",
      "         clean                    0                   None\n",
      "         clean                    0                   None\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "FILE: yellow_tripdata_2024-06_silver\n",
      "====================================================================================================\n",
      "Total Records: 3,452,076\n",
      "Total Fields: 37\n",
      "\n",
      "KEY TRIP FIELDS:\n",
      "----------------------------------------------------------------------------------------------------\n",
      " vendorid tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  pulocationid  dolocationid  ratecodeid  payment_type  fare_amount  total_amount\n",
      "        2  2024-06-10 20:29:54   2024-06-10 20:35:44              1.0           1.29           249           186         1.0             1          8.6         16.32\n",
      "        2  2024-06-10 20:39:21   2024-06-10 20:51:30              1.0           2.13            68           142         1.0             1         13.5         24.05\n",
      "        1  2024-06-10 20:53:15   2024-06-10 21:01:07              1.0           1.20           113           137         1.0             1          7.9         15.50\n",
      "        2  2024-06-10 20:08:52   2024-06-10 20:38:24              1.0           6.11            79           179         1.0             1         31.7         44.04\n",
      "        1  2024-06-10 20:03:11   2024-06-10 20:09:32              1.0           1.30            43           142         1.0             1          8.6         16.30\n",
      "\n",
      "ENRICHMENT FIELDS:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "                 vendor_name pickup_borough             pickup_zone dropoff_borough                 dropoff_zone rate_description payment_method\n",
      "               Curb Mobility      Manhattan            West Village       Manhattan Penn Station/Madison Sq West    Standard rate    Credit card\n",
      "               Curb Mobility      Manhattan            East Chelsea       Manhattan          Lincoln Square East    Standard rate    Credit card\n",
      "Creative Mobile Technologies      Manhattan Greenwich Village North       Manhattan                     Kips Bay    Standard rate    Credit card\n",
      "               Curb Mobility      Manhattan            East Village          Queens                  Old Astoria    Standard rate    Credit card\n",
      "Creative Mobile Technologies      Manhattan            Central Park       Manhattan          Lincoln Square East    Standard rate    Credit card\n",
      "\n",
      "SILVER METADATA:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "_silver_status  _silver_issue_count _silver_quality_issues\n",
      "         clean                    0                   None\n",
      "         clean                    0                   None\n",
      "         clean                    0                   None\n",
      "         clean                    0                   None\n",
      "         clean                    0                   None\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "FILE: yellow_tripdata_2024-12_silver\n",
      "====================================================================================================\n",
      "Total Records: 3,563,400\n",
      "Total Fields: 37\n",
      "\n",
      "KEY TRIP FIELDS:\n",
      "----------------------------------------------------------------------------------------------------\n",
      " vendorid tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  pulocationid  dolocationid  ratecodeid  payment_type  fare_amount  total_amount\n",
      "        2  2024-12-29 14:34:09   2024-12-29 14:50:48              4.0           5.78            45           162         1.0             2        -26.1        -30.10\n",
      "        2  2024-12-29 14:34:09   2024-12-29 14:50:48              4.0           5.78            45           162         1.0             2         26.1         30.10\n",
      "        2  2024-12-29 14:20:19   2024-12-29 14:33:09              4.0           1.23           113           100         1.0             1         12.8         21.80\n",
      "        2  2024-12-29 14:42:45   2024-12-29 15:19:53              3.0           4.79           230           125         1.0             1         35.9         47.88\n",
      "        2  2024-12-29 14:35:45   2024-12-29 15:31:52              3.0          10.38           138           161         1.0             1         60.4        100.99\n",
      "\n",
      "ENRICHMENT FIELDS:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  vendor_name pickup_borough               pickup_zone dropoff_borough     dropoff_zone rate_description payment_method\n",
      "Curb Mobility      Manhattan                 Chinatown       Manhattan     Midtown East    Standard rate           Cash\n",
      "Curb Mobility      Manhattan                 Chinatown       Manhattan     Midtown East    Standard rate           Cash\n",
      "Curb Mobility      Manhattan   Greenwich Village North       Manhattan Garment District    Standard rate    Credit card\n",
      "Curb Mobility      Manhattan Times Sq/Theatre District       Manhattan        Hudson Sq    Standard rate    Credit card\n",
      "Curb Mobility         Queens         LaGuardia Airport       Manhattan   Midtown Center    Standard rate    Credit card\n",
      "\n",
      "SILVER METADATA:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "_silver_status  _silver_issue_count                                                                                                   _silver_quality_issues\n",
      "       flagged                    5 negative_fare_amount,negative_mta_tax,negative_improvement_surcharge,negative_total_amount,negative_congestion_surcharge\n",
      "         clean                    0                                                                                                                     None\n",
      "         clean                    0                                                                                                                     None\n",
      "         clean                    0                                                                                                                     None\n",
      "         clean                    0                                                                                                                     None\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "✓ Visual verification complete!\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Visual Check: Display Top 5 Rows from Different Files\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Select sample files from different periods\n",
    "sample_files = [\n",
    "    \"/home/ubuntu/dat535-2025-group10/silver_layer/yellow_tripdata_2023-01_silver\",\n",
    "    \"/home/ubuntu/dat535-2025-group10/silver_layer/yellow_tripdata_2023-06_silver\",\n",
    "    \"/home/ubuntu/dat535-2025-group10/silver_layer/yellow_tripdata_2024-01_silver\",\n",
    "    \"/home/ubuntu/dat535-2025-group10/silver_layer/yellow_tripdata_2024-06_silver\",\n",
    "    \"/home/ubuntu/dat535-2025-group10/silver_layer/yellow_tripdata_2024-12_silver\"\n",
    "]\n",
    "\n",
    "for file_path in sample_files:\n",
    "    file_name = file_path.split(\"/\")[-1]\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"FILE: {file_name}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Load and display top 5 rows\n",
    "    df = spark.read.parquet(file_path)\n",
    "    \n",
    "    # Show record count and schema\n",
    "    print(f\"Total Records: {df.count():,}\")\n",
    "    print(f\"Total Fields: {len(df.columns)}\")\n",
    "    print()\n",
    "    \n",
    "    # Convert to Pandas for better display (top 5 rows only)\n",
    "    sample_pd = df.limit(5).toPandas()\n",
    "    \n",
    "    # Display key original fields\n",
    "    print(\"KEY TRIP FIELDS:\")\n",
    "    print(\"-\" * 100)\n",
    "    key_fields = ['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', \n",
    "                  'passenger_count', 'trip_distance', 'pulocationid', 'dolocationid',\n",
    "                  'ratecodeid', 'payment_type', 'fare_amount', 'total_amount']\n",
    "    available_key_fields = [f for f in key_fields if f in sample_pd.columns]\n",
    "    print(sample_pd[available_key_fields].to_string(index=False))\n",
    "    print()\n",
    "    \n",
    "    # Display enrichment fields\n",
    "    print(\"ENRICHMENT FIELDS:\")\n",
    "    print(\"-\" * 100)\n",
    "    enrichment_fields = ['vendor_name', 'pickup_borough', 'pickup_zone', \n",
    "                         'dropoff_borough', 'dropoff_zone', \n",
    "                         'rate_description', 'payment_method']\n",
    "    available_enrichment_fields = [f for f in enrichment_fields if f in sample_pd.columns]\n",
    "    print(sample_pd[available_enrichment_fields].to_string(index=False))\n",
    "    print()\n",
    "    \n",
    "    # Display Silver metadata\n",
    "    print(\"SILVER METADATA:\")\n",
    "    print(\"-\" * 100)\n",
    "    metadata_fields = ['_silver_status', '_silver_issue_count', '_silver_quality_issues']\n",
    "    available_metadata_fields = [f for f in metadata_fields if f in sample_pd.columns]\n",
    "    print(sample_pd[available_metadata_fields].to_string(index=False))\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"✓ Visual verification complete!\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e6ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
